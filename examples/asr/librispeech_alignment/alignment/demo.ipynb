{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tutorial on obtaining accurate speech-to-text alignment for long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial consists of two parts. \n",
    "- The first part corresponds to the paper *Less Peaky And More Accurate CTC Forced Alignment by Label Priors* published in ICASSP 2024. We will desmonstrate how to obtain more accurate speech-to-text alignment compared to a standard CTC model.\n",
    "- In the second part, we will provide a robust pytorch-based speech-to-text alignment library to align long audio and noisy text. For example, aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its [audiobook chapter](https://librivox.org/walden-by-henry-david-thoreau/) (of 30 minutes in this demo, or even longer) in the [LibriVox project](https://librivox.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import IPython\n",
    "import sys\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: obtaining more accurate CTC alignment by label priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Alignment with standard CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Alignment with CTC model with label priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Fine-tuning CTC model with label priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: obtaining robust alignment for long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 1, we performed forced alignment at the utterance level. In practice, we don't usually have a small segment (e.g., 10 seconds) of audio and its corresponding exact, verbatim transcription as in a laboratory setting (e.g. [LIBRISPEECH](https://www.openslr.org/12) corpus). Instead, the audios come in long form (e.g., a whole mp3 recording of speech for an hour). The transcription for the whole recording can by noisy and non-verbatim, which may not exactly match what's been spoken in the recording. In particular, in order to use the raw speech data for machine learning, we usually need to prepare a corpus of segmented audios. In some applications, we still hope to align the long audio and text data as much as possible. In this tutorial, we will provide a python library to support such use cases.\n",
    "\n",
    "Here, we are facing two challenges:\n",
    "- The audio is long, which may not be suitable to be handled as a whole due to, e.g., limited CPU/GPU memory.\n",
    "- The transcript is noisy. It can be a partial transcript with some missing words. It may have significant errors. It may also contain extra contents that's not spoken in the audio (e.g., the corresponding audio is corrupted). It can be a combination of all cases. Thus, the conventional, basic forced alignment algorithm could provide very bad alignment results, as it assumes the audio and text match exactly.\n",
    "\n",
    "There are a few existing solutions:\n",
    "- [Kaldi](https://ieeexplore.ieee.org/document/8268956), [Gentle](https://github.com/lowerquality/gentle) and [this work](https://ieeexplore.ieee.org/document/7404861) employ a weighted finite state transducer (WFST) framework to model the noisy texts. \n",
    "- [WhisperX](https://github.com/m-bain/whisperX) uses attention mechanism to propose rough time stamps for uniformly segmented audio. Then, it performs phone-level or word-level forced alignment with an external aligner.\n",
    "- [MMS](https://arxiv.org/abs/2305.13516) uses a special `<star>` token to handle missing words in the transcript.\n",
    "- [SailAlign](https://www.semanticscholar.org/paper/SailAlign%3A-Robust-long-speech-text-alignment-Katsamanis-Georgiou/0b7f86429641b188cc62ec32eee590e8795a3d02) iteratively identifies reliable regions and then narrows down to align the remaining unaligned regions.\n",
    "\n",
    "This tutorial is based on WFST and thus falls in the first category. Our implementation is based on PyTorch. Any CTC model in PyTorch can be equipped with our library to become a robust aligner. This makes our aligner distinguish from existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For WFST, our library depends on [k2](https://github.com/k2-fsa/k2/), a pytorch-based WFST library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check python and pytorch's version\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install k2==1.24.4.dev20240223+cuda11.7.torch2.0.1-cp310 -f https://k2-fsa.github.io/k2/cpu.html\n",
    "!pip install cmudict g2p_en\n",
    "!pip install git+https://github.com/huangruizhe/lis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone xxx\n",
    "\n",
    "sys.path.append(\"/exp/rhuang/meta/audio_latest/examples/asr/librispeech_alignment/alignment\")\n",
    "\n",
    "from torchaudio_k2_aligner import (\n",
    "    uniform_segmentation_with_overlap,\n",
    "    align_segments,\n",
    "    align,\n",
    "    align_long_text,\n",
    ")\n",
    "from tokenizer import EnglishCharTokenizer\n",
    "from factor_transducer import make_factor_transducer_word_level_index_with_skip\n",
    "from k2_icefall_utils import (\n",
    "    get_best_paths,\n",
    "    get_texts_with_timestamp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pre-trained Wav2Vec2 model, [torchaudio.pipelines.MMS_FA](https://pytorch.org/audio/main/generated/torchaudio.pipelines.MMS_FA.html#torchaudio.pipelines.MMS_FA), as the acoustic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.MMS_FA\n",
    "model = bundle.get_model(with_star=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = bundle.get_labels(star=\"*\")\n",
    "DICTIONARY = bundle.get_dict(star=\"*\")\n",
    "\n",
    "print(LABELS)\n",
    "\n",
    "tokenizer = EnglishCharTokenizer(\n",
    "    token2id=DICTIONARY,\n",
    "    blk_token=\"-\",\n",
    "    unk_token=\"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its audiobook chapter (of 30 minutes) in the [LibriVox project](https://librivox.org/walden-by-henry-david-thoreau/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the whole book\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "text = soup.get_text()\n",
    "text = text.replace(\"\\r\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a chapter of the audio book\n",
    "# !wget https://ia800707.us.archive.org/20/items/walden_librivox/walden_c07.mp3\n",
    "\n",
    "SPEECH_FILE = \"walden_c07.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the long audio\n",
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transcript\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transcript relevant to the audio\n",
    "print(text[271400: 271400+1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the audio contains a header \"This is a LibriVox recording ...\" which is not transcribed. On the other hand, as we have downloaded the whole book, it contains a lot of extra text that's not spoken in the audio. Obviously, the standard forced alignment algorithm will not work in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize the text into the labels of the acoustic model's output\n",
    "text_tokenized = tokenizer.encode(tokenizer.text_normalize(text))\n",
    "\n",
    "print(f\"There are {len(text_tokenized)} words in the text\")\n",
    "\n",
    "# Preview the tokenization results. This corresponds to the beginning of the audio\n",
    "print(tokenizer.decode(text_tokenized)[49489: 49489+15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use WFST to represent the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 WFST basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 CTC graph, factor transducer and the variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will represent the whole book of over 100K words into one single WFST decoding graph that allows occasional insertion/deletioin/substitution errors. We set `skip_penalty=-0.5` and `return_penalty=-18.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_graph, word_index_sym_tab, token_sym_tab = \\\n",
    "    make_factor_transducer_word_level_index_with_skip(\n",
    "        text_tokenized, \n",
    "        blank_penalty=0, \n",
    "        skip_penalty=-0.5, \n",
    "        return_penalty=-18.0\n",
    "    )\n",
    "decoding_graph = decoding_graph.to(device)\n",
    "\n",
    "print(f\"There are {decoding_graph.shape[0]} nodes and {decoding_graph.num_arcs} arcs in the decoding graph for the text of {len(text_tokenized)} words.\")\n",
    "print(f\"The decoding graph is on {decoding_graph.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle long audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope to feed the audio into the neural network to get the frame-wise posteriors (i.e., the emission matrix) over the label vocabulary. However, the audio book chapter is of about 30 minutes. This is too long to feed into the acoustic model all at once. \n",
    "\n",
    "A common practice is to segment the long audio into small overlapping segments. The segments are processed independently and the results are concatenated appropriately to make the final alignment result. Here, as the Wav2Vec2 pretrained model that we use is taking a raw wave form as input, so we will segment the original audio into 15-second segments. In practice, we can also segment the feature vector (e.g., Fbanks) to feed into the acoustic model.\n",
    "\n",
    "The library that we provide in this tutorial provides functions for appropriate segmentation and concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the library to segment the 30-minute audio into 15-second segments, with an overlap of 2 seconds between neighboring segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "resample_rate = 16000  # this is the sample rate of the Wav2Vec2 model\n",
    "waveform = torchaudio.functional.resample(waveform, sample_rate, resample_rate)\n",
    "sample_rate = resample_rate\n",
    "print(waveform.shape, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio_k2_aligner\n",
    "\n",
    "import importlib\n",
    "importlib.reload(torchaudio_k2_aligner)\n",
    "\n",
    "uniform_segmentation_with_overlap = torchaudio_k2_aligner.uniform_segmentation_with_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if waveform.dim() == 2:\n",
    "    waveform.unsqueeze_(-1)\n",
    "segment_size = sample_rate * 15 + 128  # 15 seconds; use extra 128 waveform samples to make sure we have 750 frames for each full-sized segment\n",
    "overlap = sample_rate * 2 + 128        # 2 seconds\n",
    "shortest_segment_size = sample_rate * 0.2  # if the last segment has less than 3200 samples (0.2 seconds), it will be discarded\n",
    "\n",
    "waveform_segmented, segment_lengths, segment_offsets = uniform_segmentation_with_overlap(\n",
    "    waveform,\n",
    "    segment_size, \n",
    "    overlap, \n",
    "    shortest_segment_size=shortest_segment_size\n",
    ")\n",
    "waveform_segmented = waveform_segmented.squeeze()\n",
    "print(waveform_segmented.shape, segment_lengths.shape, segment_offsets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can listen to a segment to make sure it is correct\n",
    "IPython.display.Audio(waveform_segmented[-1], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Obtain alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.device(\"cpu\"):\n",
    "    batch_size = 2\n",
    "else:\n",
    "    batch_size = 32\n",
    "\n",
    "results_ids = list()\n",
    "results_timestamps = list()\n",
    "for i in tqdm(range(0, waveform_segmented.size(0), batch_size)):\n",
    "    batch_waveform_segmented = waveform_segmented[i: i+batch_size].to(device)\n",
    "    batch_segment_lengths = segment_lengths[i: i+batch_size]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Checkout the API of the forward function here: https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/utils.py#L34\n",
    "        batch_emissions, batch_emissions_lengths = model(batch_waveform_segmented.to(device), batch_segment_lengths.to(device))\n",
    "    \n",
    "    # Attach the star dimension manually, see torchaudio issue #3772\n",
    "    star_dim = torch.empty((batch_emissions.size(0), batch_emissions.size(1), 1), device=batch_emissions.device, dtype=batch_emissions.dtype)\n",
    "    star_dim[:] = -5.0\n",
    "    batch_emissions = torch.cat((batch_emissions, star_dim), 2)\n",
    "\n",
    "    # `token_ids` and `timestamps` will each be a list of lists.\n",
    "    # Each sublist corresponds to a segment in the batch.\n",
    "    token_ids, timestamps = align_segments(\n",
    "        batch_emissions,\n",
    "        decoding_graph,\n",
    "        batch_emissions_lengths,\n",
    "    )\n",
    "\n",
    "    # We need to convert token_ids to word index in the original text\n",
    "    word_ids = []\n",
    "    word_timestamps = []\n",
    "    for _token_ids, _timestamps in zip(token_ids, timestamps):\n",
    "        _word_ids = []\n",
    "        _word_timestamps = []\n",
    "        for token_id, timestamp in zip(_token_ids, _timestamps):\n",
    "            if token_id in word_index_sym_tab:\n",
    "                _word_ids.append(word_index_sym_tab[token_id])\n",
    "                _word_timestamps.append(timestamp)\n",
    "        word_ids.append(_word_ids)\n",
    "        word_timestamps.append(_word_timestamps)\n",
    "\n",
    "    results_ids.extend(word_ids)\n",
    "    results_timestamps.extend(word_timestamps)\n",
    "\n",
    "    # It will be too slow to do inference on CPU. \n",
    "    # As it's only for the demonstration purpose, so we will break the loop here.\n",
    "    # We can still see some partial alignment results.\n",
    "    if device == torch.device(\"cpu\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"results_ids\": results_ids,\n",
    "        \"results_timestamps\": results_timestamps,\n",
    "    },\n",
    "    \"results.pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frames_offset = segment_offsets // (sample_rate * 0.02)\n",
    "\n",
    "rs = {\n",
    "    \"meta_data\": None,\n",
    "    \"hyps\": results_ids,\n",
    "    \"timestamps\": results_timestamps,\n",
    "    \"output_frame_offset\": output_frames_offset,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# The beginning and ending of the audio is hard to be aligned to the book.\n",
    "# We might need to use VAD or something to handle it. Or predefine the start/end of the audio and text.\n",
    "# Ignored them for now cos we just use the alignment for ASR training.\n",
    "\n",
    "# `alignment_results` is a dict: \n",
    "#   word_index (in the long text) => output frame_index (in the long audio), which means is downsampled by params.subsampling_factor\n",
    "# `to_realign` is a list of (start_word_index, end_word_index)\n",
    "#   where both ends are already aligned, which means they are in `alignment_results`\n",
    "\n",
    "alignment_results, to_realign = align_long_text(\n",
    "    rs, \n",
    "    num_segments_per_chunk=5,\n",
    "    neighbor_threshold=5,\n",
    "    device=device,\n",
    ")\n",
    "len(alignment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audacity_labels_str = to_audacity_label_format(alignment_results, 0.02, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audacity_path = str(SPEECH_FILE)[:-4] + \"_audacity.txt\"\n",
    "with open(audacity_path, \"w\") as fout:\n",
    "    print(audacity_labels_str, file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f\"{x:5}\" for x in results_ids[1]])\n",
    "print([f\"{x:5}\" for x in results_timestamps[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frames_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lis\n",
    "from torchaudio_k2_aligner import *\n",
    "\n",
    "hyps = rs['hyps']\n",
    "timestamps = rs['timestamps']\n",
    "output_frame_offset = rs['output_frame_offset'].tolist()\n",
    "meta_data = rs['meta_data']\n",
    "\n",
    "alignment_results = dict()\n",
    "\n",
    "# find the longest increasing subsequence\n",
    "hyp_list = [i for hyp in hyps for i in hyp]\n",
    "max_symbol_id = max(hyp_list) + 100   # use this symbol to mark the chunk boundaries\n",
    "hyp_list = [i for hyp in hyps for i in [max_symbol_id] + hyp]\n",
    "hyp_list = hyp_list[1:]\n",
    "lis_result = lis.longestIncreasingSubsequence(hyp_list)\n",
    "lis_result = remove_outliers(lis_result, max_symbol_id, scan_range=100, outlier_threshold=60)\n",
    "indices_in_segment = get_lis_alignment(hyp_list, lis_result, max_symbol_id)  # hyp_list and lis_result are both word indices in the long text\n",
    "\n",
    "for idx, ts, ofs in zip(indices_in_segment, timestamps, output_frame_offset):\n",
    "    for k, v in idx.items():\n",
    "        alignment_results[k] = ts[v] + ofs\n",
    "\n",
    "len(alignment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_in_segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyps[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tki = 1\n",
    "token_ids_indices[tki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join([tokenizer.id2token[token_sym_tab[i]] for i in token_ids_indices[tki]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word_index_sym_tab[i] for i in token_ids_indices[tki] if i in word_index_sym_tab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitted = text.split()\n",
    "[text_splitted[word_index_sym_tab[i]] for i in token_ids_indices[tki] if i in word_index_sym_tab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k2\n",
    "import torch\n",
    "\n",
    "\n",
    "def uniform_segmentation_with_overlap(m, segment_size, overlap, shortest_segment_size=0):\n",
    "    '''\n",
    "    This function cuts the input matrix `m` into overlapping segments.\n",
    "    `m` can be, e.g., the feature matrix of the input audio or the emission matrix.\n",
    "\n",
    "    Args:\n",
    "        m: 3-D tensor of shape (1, T, D).\n",
    "        segment_size: an integer, the size of each segment.\n",
    "        overlap: an integer, the number of frames to overlap between segments.\n",
    "    '''\n",
    "    assert m.ndim == 3\n",
    "    assert m.size(0) == 1\n",
    "    # m = m.unsqueeze(0)  # (1, T, D)\n",
    "\n",
    "    step = segment_size - overlap\n",
    "    if (m.size(1) - segment_size) % step == 0:\n",
    "        n_segments = (m.size(1) - segment_size) // step + 1\n",
    "        padding_size = 0\n",
    "    else:\n",
    "        n_segments = (m.size(1) - segment_size) // step + 2\n",
    "        padding_size = (n_segments - 1) * step + segment_size - m.size(1)\n",
    "    \n",
    "    m_padded = torch.nn.functional.pad(m, (0, 0, 0, padding_size))  # Pad the tensor with zeros\n",
    "    m_segmented = m_padded.unfold(dimension=1, size=segment_size, step=step)\n",
    "    m_segmented = m_segmented.permute(0, 1, 3, 2)  # `m_segmented` is of shape (1, L, segment_size, D), where L is the number of segments.\n",
    "    m_segmented = m_segmented[0]  # (L, segment_size, D)\n",
    "    \n",
    "    segment_lengths = [segment_size] * m_segmented.size(0)\n",
    "    segment_lengths[-1] -= padding_size\n",
    "    segment_lengths = torch.tensor(segment_lengths)\n",
    "\n",
    "    segment_offsets = torch.arange(0, segment_lengths.size(0) * step, step)\n",
    "\n",
    "    assert len(segment_lengths) == n_segments\n",
    "    assert len(segment_lengths) == m_segmented.size(0)\n",
    "    assert len(segment_lengths) == len(segment_offsets)\n",
    "\n",
    "    # Discard the last chunk if it is too short\n",
    "    if segment_lengths[-1] < shortest_segment_size:\n",
    "        m_segmented = m_segmented[:-1]\n",
    "        segment_lengths = segment_lengths[:-1]\n",
    "        segment_offsets = segment_offsets[:-1]\n",
    "    \n",
    "    return m_segmented, segment_lengths, segment_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.rand(1, 20, 3)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_segmentation_with_overlap(m, 7, 2, shortest_segment_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.rand(1, 7, 3)\n",
    "segment_size = 7\n",
    "overlap = 2\n",
    "step = segment_size - overlap\n",
    "if (m.size(1) - segment_size) % step == 0:\n",
    "    n_segments = (m.size(1) - segment_size) // step + 1\n",
    "    padding_size = 0\n",
    "else:\n",
    "    n_segments = (m.size(1) - segment_size) // step + 2\n",
    "    padding_size = (n_segments - 1) * step + segment_size - m.size(1)\n",
    "\n",
    "print(n_segments, padding_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
