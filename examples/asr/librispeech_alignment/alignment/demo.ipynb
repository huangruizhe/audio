{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tutorial on obtaining accurate speech-to-text alignment for long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial consists of two parts. \n",
    "- The first part corresponds to the paper *Less Peaky And More Accurate CTC Forced Alignment by Label Priors* published in ICASSP 2024. We will desmonstrate how to obtain more accurate speech-to-text alignment compared to a standard CTC model.\n",
    "- In the second part, we will provide a robust pytorch-based speech-to-text alignment library to align long audio and noisy text. For example, aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its [audiobook chapter](https://librivox.org/walden-by-henry-david-thoreau/) (of 30 minutes in this demo, or even longer) in the [LibriVox project](https://librivox.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import IPython\n",
    "import sys\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: obtaining more accurate CTC alignment by label priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Alignment with standard CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Alignment with CTC model with label priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Fine-tuning CTC model with label priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be added. Basically, we will show this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: obtaining robust alignment for long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 1, we performed forced alignment at the utterance level. In practice, we don't usually have a small segment (e.g., 10 seconds) of audio and its corresponding exact, verbatim transcription as in a laboratory setting (e.g. [LIBRISPEECH](https://www.openslr.org/12) corpus). Instead, the audios come in long form (e.g., a whole mp3 recording of speech for an hour). The transcription for the whole recording can by noisy and non-verbatim, which may not exactly match what's been spoken in the recording. In particular, in order to use the raw speech data for machine learning, we usually need to prepare a corpus of segmented audios. In some applications, we still hope to align the long audio and text data as much as possible. In this tutorial, we will provide a python library to support such use cases.\n",
    "\n",
    "Here, we are facing two challenges:\n",
    "- **The audio is long**, which may not be suitable to be handled as a whole due to, e.g., limited CPU/GPU memory.\n",
    "- **The transcript is noisy**. It can be a partial transcript with some missing words. It may have significant errors. It may also contain extra contents that's not spoken in the audio (e.g., the corresponding audio is corrupted). It can be a combination of all cases. Thus, the conventional, basic forced alignment algorithm could provide very bad alignment results, as it assumes the audio and text match exactly.\n",
    "\n",
    "There are a few existing solutions:\n",
    "- [Kaldi](https://ieeexplore.ieee.org/document/8268956), [Gentle](https://github.com/lowerquality/gentle) and [this work](https://ieeexplore.ieee.org/document/7404861) employ a weighted finite state transducer (WFST) framework to model the noisy texts. \n",
    "- [WhisperX](https://github.com/m-bain/whisperX) uses attention mechanism to propose rough time stamps for uniformly segmented audio. Then, it performs phone-level or word-level forced alignment with an external aligner.\n",
    "- [MMS](https://arxiv.org/abs/2305.13516) uses a special `<star>` token to handle missing words in the transcript.\n",
    "- [SailAlign](https://www.semanticscholar.org/paper/SailAlign%3A-Robust-long-speech-text-alignment-Katsamanis-Georgiou/0b7f86429641b188cc62ec32eee590e8795a3d02) iteratively identifies reliable regions and then narrows down to align the remaining unaligned regions.\n",
    "\n",
    "This tutorial is based on WFST and thus falls in the first category. Our implementation is based on PyTorch. Any CTC model in PyTorch can be equipped with our library to become a robust aligner. This makes our aligner distinguish from existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For WFST, our library depends on [k2](https://github.com/k2-fsa/k2/), a pytorch-based WFST library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check python and pytorch's version\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k2 (cpu)\n",
    "# pip install k2==1.24.4.dev20240223+cpu.torch2.2.1 -f https://k2-fsa.github.io/k2/cpu.html\n",
    "# k2 (gpu)\n",
    "!pip install k2==1.24.4.dev20240301+cuda12.1.torch2.2.1 -f https://k2-fsa.github.io/k2/cuda.html\n",
    "!pip install cmudict g2p_en\n",
    "!pip install git+https://github.com/huangruizhe/lis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone xxx\n",
    "\n",
    "sys.path.append(\"/exp/rhuang/meta/audio_latest/examples/asr/librispeech_alignment/alignment\")\n",
    "\n",
    "from torchaudio_k2_aligner import (\n",
    "    uniform_segmentation_with_overlap,\n",
    "    align_segments,\n",
    "    concat_alignments,\n",
    "    get_final_word_alignment,\n",
    "    align,\n",
    ")\n",
    "from tokenizer import EnglishCharTokenizer\n",
    "from factor_transducer import make_factor_transducer_word_level_index_with_skip\n",
    "from k2_icefall_utils import (\n",
    "    get_best_paths,\n",
    "    get_texts_with_timestamp,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pre-trained Wav2Vec2 model, [torchaudio.pipelines.MMS_FA](https://pytorch.org/audio/main/generated/torchaudio.pipelines.MMS_FA.html#torchaudio.pipelines.MMS_FA), as the acoustic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.MMS_FA\n",
    "model = bundle.get_model(with_star=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = bundle.get_labels(star=\"*\")\n",
    "DICTIONARY = bundle.get_dict(star=\"*\")\n",
    "\n",
    "print(LABELS)\n",
    "\n",
    "tokenizer = EnglishCharTokenizer(\n",
    "    token2id=DICTIONARY,\n",
    "    blk_token=\"-\",\n",
    "    unk_token=\"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its audiobook chapter (of 30 minutes) in the [LibriVox project](https://librivox.org/walden-by-henry-david-thoreau/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the whole book\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "text = soup.get_text()\n",
    "text = text.replace(\"\\r\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a chapter of the audio book\n",
    "# !wget https://ia800707.us.archive.org/20/items/walden_librivox/walden_c07.mp3\n",
    "\n",
    "SPEECH_FILE = \"walden_c07.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the long audio\n",
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transcript\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transcript relevant to the audio\n",
    "\n",
    "# In this running example, the whole audio corresponds to \n",
    "# text[271616: 293529]\n",
    "# or\n",
    "# text.split()[49489: 53362]\n",
    "\n",
    "print(text[271400: 271400+1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the audio contains a header \"This is a LibriVox recording ...\" which is not transcribed. On the other hand, as we have downloaded the whole book, it contains a lot of extra text that's not spoken in the audio. Obviously, the standard forced alignment algorithm will not work in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tokenize the text into the labels of the acoustic model's output\n",
    "text_tokenized = tokenizer.encode(tokenizer.text_normalize(text))\n",
    "\n",
    "print(f\"There are {len(text_tokenized)} words in the text\")\n",
    "\n",
    "# Preview the tokenization results. This corresponds to the beginning of the audio\n",
    "print(tokenizer.decode(text_tokenized)[49489: 49489+15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use WFST to represent the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 WFST basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 CTC graph, factor transducer and the variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will represent the whole book of over 100K words into one single WFST decoding graph that allows occasional insertion/deletioin/substitution errors. We set `skip_penalty=-0.5` and `return_penalty=-18.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding_graph, word_index_sym_tab, token_sym_tab = \\\n",
    "    make_factor_transducer_word_level_index_with_skip(\n",
    "        text_tokenized, \n",
    "        blank_penalty=0, \n",
    "        skip_penalty=-0.5, \n",
    "        return_penalty=-18.0\n",
    "    )\n",
    "decoding_graph = decoding_graph.to(device)\n",
    "\n",
    "print(f\"There are {decoding_graph.shape[0]} nodes and {decoding_graph.num_arcs} arcs in the decoding graph for the text of {len(text_tokenized)} words.\")\n",
    "print(f\"The decoding graph is on device: {decoding_graph.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle long audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope to feed the audio into the neural network to get the frame-wise posteriors (i.e., the emission matrix) over the label vocabulary. However, the audio book chapter is of about 30 minutes. This is too long to feed into the acoustic model all at once. \n",
    "\n",
    "A common practice is to segment the long audio into small overlapping segments. The segments are processed independently and the results are concatenated appropriately to make the final alignment result. Here, as the Wav2Vec2 pretrained model that we use is taking a raw wave form as input, so we will segment the original audio into 15-second segments. In practice, we can also segment the feature vector (e.g., Fbanks) to feed into the acoustic model.\n",
    "\n",
    "The library that we provide in this tutorial provides functions for appropriate segmentation and concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the library to segment the 30-minute audio into 15-second segments, with an overlap of 2 seconds between neighboring segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "resample_rate = 16000  # this is the sample rate of the Wav2Vec2 model\n",
    "waveform = torchaudio.functional.resample(waveform, sample_rate, resample_rate)\n",
    "sample_rate = resample_rate\n",
    "print(waveform.shape, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if waveform.dim() == 2:\n",
    "    waveform.unsqueeze_(-1)\n",
    "segment_size = sample_rate * 15 + 128  # 15 seconds; use extra 128 waveform samples to make sure we have 750 frames for each full-sized segment\n",
    "overlap = sample_rate * 2 + 128        # 2 seconds\n",
    "shortest_segment_size = sample_rate * 0.2  # if the last segment has less than 3200 samples (0.2 seconds), it will be discarded\n",
    "\n",
    "waveform_segmented, segment_lengths, segment_offsets = uniform_segmentation_with_overlap(\n",
    "    waveform,\n",
    "    segment_size, \n",
    "    overlap, \n",
    "    shortest_segment_size=shortest_segment_size\n",
    ")\n",
    "waveform_segmented = waveform_segmented.squeeze()\n",
    "print(waveform_segmented.shape, segment_lengths.shape, segment_offsets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can listen to a segment to make sure the segmentation pipeline runs correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(waveform_segmented[0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Obtain alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will feed the short segments to the neural network based acoustic model. We do this in batches. For each segment, the acoustic model produces frame-wise classification `batch_emissions` over its label vocabulary. This is then combined with the WFST `decoding_graph`, which is shared by all segments, to produce the best alignment path for each segment. If the `decoding_graph` is a linear WFST, which means there is no insertion/deletion/substitution errors in the transcript, this is equivalent to the conventional forced alignment as provided by this [TorchAudio API](https://pytorch.org/audio/main/tutorials/ctc_forced_alignment_api_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There something to note for the `decoding_graph`. When we combine it with the neural network output, its \"input label\" should match the network's label vocabulary. However, it is our decision to define decoding graph's \"output labels\", which basically ask this question: what do we align the audio to? Here are two examples of design choices:\n",
    "\n",
    "- The output labels are word labels or phoneme labels, as in most WFST-based alignment solutions (e.g., in [Gentle](https://github.com/lowerquality/gentle/blob/master/gentle/diff_align.py#L16)). In this case, the alignment paths consist of the word-level or phoneme-level transcripts predicted by the model.\n",
    "\n",
    "- The output labels are the **word indices** in the transcript, instead of word labels. There are two benefits: (1) we can obtain the word labels easily from the word indices; (2) with word indices, we are able to preserve the ordering of words in the transcript, even though the alignment paths may contain words in the order different from the original transcript. As we will see later, this word ordering information enables efficient and effective heuristics for post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == torch.device(\"cpu\"):\n",
    "    batch_size = 2\n",
    "else:\n",
    "    batch_size = 32\n",
    "\n",
    "output_frames_offset = segment_offsets // (sample_rate * 0.02)\n",
    "\n",
    "alignment_results = list()\n",
    "for i in tqdm(range(0, waveform_segmented.size(0), batch_size)):\n",
    "    batch_waveform_segmented = waveform_segmented[i: i+batch_size].to(device)\n",
    "    batch_segment_lengths = segment_lengths[i: i+batch_size]\n",
    "    batch_output_frames_offset = output_frames_offset[i: i+batch_size]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Checkout the API of the forward function here: https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/utils.py#L34\n",
    "        batch_emissions, batch_emissions_lengths = model(batch_waveform_segmented.to(device), batch_segment_lengths.to(device))\n",
    "    \n",
    "    # Attach the star dimension manually, see torchaudio issue #3772\n",
    "    star_dim = torch.empty((batch_emissions.size(0), batch_emissions.size(1), 1), device=batch_emissions.device, dtype=batch_emissions.dtype)\n",
    "    star_dim[:] = -5.0\n",
    "    batch_emissions = torch.cat((batch_emissions, star_dim), 2)\n",
    "\n",
    "    # `token_ids` and `timestamps` will each be a list of lists.\n",
    "    # Each sublist corresponds to a segment in the batch.\n",
    "    batch_results = align_segments(\n",
    "        batch_emissions,\n",
    "        decoding_graph,\n",
    "        batch_emissions_lengths,\n",
    "    )\n",
    "\n",
    "    # The interpretation of `token.token_id` depends on the decoding graph.\n",
    "    # Here, in this tutorial, `token.token_id` is the key to the `word_index_sym_tab``\n",
    "    # and `token_sym_tab` dictionaries.\n",
    "    for aligned_tokens, offset in zip(batch_results, batch_output_frames_offset):\n",
    "        for token in aligned_tokens:\n",
    "            token.timestamp += offset  # This will become the absolute frame timestamp in the whole audio\n",
    "            if token.token_id == tokenizer.blk_id:\n",
    "                continue\n",
    "            if token.token_id in word_index_sym_tab:\n",
    "                token.attr[\"wid\"] = word_index_sym_tab[token.token_id]\n",
    "            if token.token_id in token_sym_tab:\n",
    "                token.attr[\"tk\"] = token_sym_tab[token.token_id]            \n",
    "\n",
    "    alignment_results.extend(batch_results)\n",
    "\n",
    "    # It will be too slow to do inference on CPU. \n",
    "    # As it's only for the demonstration purpose, so we will break the loop here.\n",
    "    # We can still see some partial alignment results.\n",
    "    if device == torch.device(\"cpu\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got the alignment results for all 140 15-second segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(alignment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to concatenate the alignment results appropriately. We need to take care of two things: (1) the mis-aligned results, (2) the overlapping parts. The common algorithm for this concatenation step is through [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) (e.g., in [Gentle](https://github.com/lowerquality/gentle/blob/master/gentle/diff_align.py#L16)), which aligns the forced-aligned transcript with the ground-truth transcript. As we have word indices instead of word labels, we will do this a bit differently. Given the alignment results for all segments, we will find the [longest increasing subsequence (LIS)](https://en.wikipedia.org/wiki/Longest_increasing_subsequence) in the word indices. This can be done in $O(NlogN)$ time complexity and $O(N)$ space complexity, instead of $O(N^2)$ by Levenshtein distance, where $N$ is the total length of the segment-wise alignment results. This can be useful especially when $N$ is large for long audios.\n",
    "\n",
    "Moreover, with word indices, we are able to develop heuristics to remove outliers in the alignment results. This has been all implemented in our alignment library. E.g., by calling the `concat_alignments` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# The beginning and ending of the audio is hard to be aligned to the book.\n",
    "# We might need to use VAD or something to handle it. Or predefine the start/end of the audio and text.\n",
    "# Ignored them for now cos we just use the alignment for ASR training.\n",
    "\n",
    "# `resolved_alignment_results` is a list of `AlignedToken`\n",
    "# `unaligned_text_indices` is a list of (start_word_index, end_word_index)\n",
    "#    which corresponds to \"holes\" in the long text that are not aligned\n",
    "\n",
    "resolved_alignment_results, unaligned_text_indices = concat_alignments(\n",
    "    alignment_results, \n",
    "    neighborhood_size=5,\n",
    ")\n",
    "\n",
    "len(resolved_alignment_results), len(unaligned_text_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final word-level alignment results can be obtained by the following, where `word_alignment` is a dictionary from word indices in `text` to an `AlignedWord` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_alignment = get_final_word_alignment(resolved_alignment_results, text, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's preview the alignment results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word_alignment.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(word_alignment.items())[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, the alignment results look pretty good! Despite the book being long and noisy (e.g., ), it successfully locates [the chapter](https://www.gutenberg.org/cache/epub/205/pg205-images.html#chap08) from the whole book corresponding to the audio, except the first two words (\"The Bean-Field\") which are the chapter title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `unaligned_text_indices`, we can see the parts in the book chapter that are not successfully aligned to the audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [s, e[] are in a \"hole\" in the long text that is not aligned\n",
    "for s, e in unaligned_text_indices:\n",
    "    if e-s > 0\n",
    "[x2-x1  if x2-x1 > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitted = text.split()\n",
    "[text_splitted[tk.attr['wid']] if 'wid' in tk.attr else tokenizer.id2token[tk.attr['tk']] for tk in resolved_alignment_results[-40:-1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolved_alignment_results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audacity_labels_str = to_audacity_label_format(alignment_results, 0.02, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audacity_path = str(SPEECH_FILE)[:-4] + \"_audacity.txt\"\n",
    "with open(audacity_path, \"w\") as fout:\n",
    "    print(audacity_labels_str, file=fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_alignment_results = alignment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([f\"{x:5}\" for x in results_ids[1]])\n",
    "print([f\"{x:5}\" for x in results_timestamps[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_frames_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lis\n",
    "from torchaudio_k2_aligner import *\n",
    "\n",
    "hyps = [[token.attr[\"wid\"] for token in aligned_tokens if \"wid\" in token.attr] for aligned_tokens in alignment_results]\n",
    "timestamps = [[token.timestamp for token in aligned_tokens if \"wid\" in token.attr] for aligned_tokens in alignment_results]\n",
    "num_segments_per_chunk=5\n",
    "neighbor_threshold=5\n",
    "device=device\n",
    "\n",
    "# Find the longest increasing subsequence (LIS)\n",
    "hyp_list = [i for hyp in hyps for i in hyp]\n",
    "lis_results = lis.longestIncreasingSubsequence(hyp_list)\n",
    "\n",
    "# Post-process1: remove outliers from the LIS results\n",
    "lis_results = remove_outliers(lis_results, scan_range=100, outlier_threshold=60)\n",
    "print(lis_results)\n",
    "print(len(lis_results))\n",
    "\n",
    "# Post-process2: remove isolatedly aligned words\n",
    "# Each aligned word should have a neighborhood of at least neighbor_threshold words\n",
    "rg_min = min(lis_results)\n",
    "rg_max = max(lis_results)\n",
    "set_lis_results = set(lis_results)\n",
    "for i in range(rg_min, rg_max + 1):\n",
    "    if i in set_lis_results:\n",
    "        left_neighbors_in_lis = [j for j in range(i-neighbor_threshold, i) if j in set_lis_results]\n",
    "        right_neighbors_in_lis = [j for j in range(i+1, i+neighbor_threshold+1) if j in set_lis_results]\n",
    "        num_left_neighbors = i - max(i-neighbor_threshold, rg_min)\n",
    "        num_right_neighbors = min(i+neighbor_threshold, rg_max) - i\n",
    "        # only less than 50% of the words in the neighborhood are aligned\n",
    "        if len(left_neighbors_in_lis) < 0.4 * num_left_neighbors and \\\n",
    "            len(right_neighbors_in_lis) < 0.4 * num_right_neighbors:\n",
    "            set_lis_results.remove(i)\n",
    "lis_results = [i for i in lis_results if i in set_lis_results]\n",
    "print(lis_results)\n",
    "print(len(lis_results))\n",
    "\n",
    "# Align LIS results with the original `alignment_results`\n",
    "alignment_results = get_lis_alignment(lis_results, alignment_results)  # hyp_list and lis_result are both word indices in the long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the aligned tokens which are in LIS\n",
    "resolved_alignment_results = list()\n",
    "for aligned_tokens in alignment_results:\n",
    "    word_start_flag = False\n",
    "    for token in aligned_tokens:\n",
    "        if token.attr.get(\"lis\", False):\n",
    "            resolved_alignment_results.append(token)\n",
    "            word_start_flag = True\n",
    "        elif \"wid\" in token.attr:\n",
    "            # assert \"lis\" not in token.attr\n",
    "            word_start_flag = False\n",
    "        elif word_start_flag:\n",
    "            resolved_alignment_results.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(resolved_alignment_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio_k2_aligner\n",
    "\n",
    "import importlib\n",
    "importlib.reload(torchaudio_k2_aligner)\n",
    "\n",
    "concat_alignments = torchaudio_k2_aligner.concat_alignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tki = 1\n",
    "token_ids_indices[tki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\".join([tokenizer.id2token[token_sym_tab[i]] for i in token_ids_indices[tki]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[word_index_sym_tab[i] for i in token_ids_indices[tki] if i in word_index_sym_tab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitted = text.split()\n",
    "[text_splitted[word_index_sym_tab[i]] for i in token_ids_indices[tki] if i in word_index_sym_tab]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import k2\n",
    "import torch\n",
    "\n",
    "\n",
    "def uniform_segmentation_with_overlap(m, segment_size, overlap, shortest_segment_size=0):\n",
    "    '''\n",
    "    This function cuts the input matrix `m` into overlapping segments.\n",
    "    `m` can be, e.g., the feature matrix of the input audio or the emission matrix.\n",
    "\n",
    "    Args:\n",
    "        m: 3-D tensor of shape (1, T, D).\n",
    "        segment_size: an integer, the size of each segment.\n",
    "        overlap: an integer, the number of frames to overlap between segments.\n",
    "    '''\n",
    "    assert m.ndim == 3\n",
    "    assert m.size(0) == 1\n",
    "    # m = m.unsqueeze(0)  # (1, T, D)\n",
    "\n",
    "    step = segment_size - overlap\n",
    "    if (m.size(1) - segment_size) % step == 0:\n",
    "        n_segments = (m.size(1) - segment_size) // step + 1\n",
    "        padding_size = 0\n",
    "    else:\n",
    "        n_segments = (m.size(1) - segment_size) // step + 2\n",
    "        padding_size = (n_segments - 1) * step + segment_size - m.size(1)\n",
    "    \n",
    "    m_padded = torch.nn.functional.pad(m, (0, 0, 0, padding_size))  # Pad the tensor with zeros\n",
    "    m_segmented = m_padded.unfold(dimension=1, size=segment_size, step=step)\n",
    "    m_segmented = m_segmented.permute(0, 1, 3, 2)  # `m_segmented` is of shape (1, L, segment_size, D), where L is the number of segments.\n",
    "    m_segmented = m_segmented[0]  # (L, segment_size, D)\n",
    "    \n",
    "    segment_lengths = [segment_size] * m_segmented.size(0)\n",
    "    segment_lengths[-1] -= padding_size\n",
    "    segment_lengths = torch.tensor(segment_lengths)\n",
    "\n",
    "    segment_offsets = torch.arange(0, segment_lengths.size(0) * step, step)\n",
    "\n",
    "    assert len(segment_lengths) == n_segments\n",
    "    assert len(segment_lengths) == m_segmented.size(0)\n",
    "    assert len(segment_lengths) == len(segment_offsets)\n",
    "\n",
    "    # Discard the last chunk if it is too short\n",
    "    if segment_lengths[-1] < shortest_segment_size:\n",
    "        m_segmented = m_segmented[:-1]\n",
    "        segment_lengths = segment_lengths[:-1]\n",
    "        segment_offsets = segment_offsets[:-1]\n",
    "    \n",
    "    return m_segmented, segment_lengths, segment_offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.rand(1, 20, 3)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_segmentation_with_overlap(m, 7, 2, shortest_segment_size=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.rand(1, 7, 3)\n",
    "segment_size = 7\n",
    "overlap = 2\n",
    "step = segment_size - overlap\n",
    "if (m.size(1) - segment_size) % step == 0:\n",
    "    n_segments = (m.size(1) - segment_size) // step + 1\n",
    "    padding_size = 0\n",
    "else:\n",
    "    n_segments = (m.size(1) - segment_size) // step + 2\n",
    "    padding_size = (n_segments - 1) * step + segment_size - m.size(1)\n",
    "\n",
    "print(n_segments, padding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- dataclass for alignment results\n",
    "- inspect the improvement for walden\n",
    "- phone-level alignment\n",
    "- gentle\n",
    "- quasi-gentle in colab\n",
    "- training recipe: need to start it tonight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
