{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A tutorial on obtaining accurate speech-to-text alignment for long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial consists of two parts. \n",
    "- The first part corresponds to the paper *Less Peaky And More Accurate CTC Forced Alignment by Label Priors* published in ICASSP 2024. We will desmonstrate how to obtain more accurate speech-to-text alignment compared to a standard CTC model.\n",
    "- In the second part, we will provide a robust pytorch-based speech-to-text alignment library to align long audio and noisy text. We will replicate the data preparation step for preparing the [Librispeech corpus](https://www.openslr.org/12). For example, aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its [audiobook chapter](https://librivox.org/walden-by-henry-david-thoreau/) (of 30 minutes in this demo, or even longer) in the [LibriVox project](https://librivox.org/).\n",
    "\n",
    "20240417: This tutorial is still under development. This is the github repository: [https://github.com/huangruizhe/audio/tree/aligner_label_priors/examples/asr/librispeech_alignment](https://github.com/huangruizhe/audio/tree/aligner_label_priors/examples/asr/librispeech_alignment). If you have any questions, please feel free to contact me (huangruizhe09@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1\n",
      "2.2.0a0+0e6ffd5\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchaudio.__version__)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "import IPython\n",
    "import sys\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: obtaining more accurate CTC alignment by label priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Alignment with standard CTC model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Alignment with CTC model with label priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Fine-tuning CTC model with label priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be added. Basically, we will replicate the results in the paper in this part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: obtaining robust alignment for long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In part 1, we performed forced alignment at the utterance level. In practice, we don't usually have a small segment (e.g., 10 seconds) of audio and its corresponding exact, verbatim transcription as in a laboratory setting (e.g. [LIBRISPEECH](https://www.openslr.org/12) corpus). Instead, the audios come in long form (e.g., a whole mp3 recording of speech for an hour). The transcription for the whole recording can by noisy and non-verbatim, which may not exactly match what's been spoken in the recording. In particular, in order to use the raw speech data for machine learning, we usually need to prepare a corpus of segmented audios. In some applications, we still hope to align the long audio and text data as much as possible. In this tutorial, we will provide a python library to support such use cases.\n",
    "\n",
    "Here, we are facing two challenges:\n",
    "- **The audio is long**, which may not be suitable to be handled as a whole due to, e.g., limited CPU/GPU memory.\n",
    "- **The transcript is noisy**. It can be a partial transcript with some missing words. It may have significant errors. It may also contain extra contents that's not spoken in the audio (e.g., the corresponding audio is corrupted). It can be a combination of all cases. Thus, the conventional, basic forced alignment algorithm could provide very bad alignment results, as it assumes the audio and text match exactly.\n",
    "\n",
    "There are a few existing solutions:\n",
    "- [Kaldi](https://ieeexplore.ieee.org/document/8268956), [Gentle](https://github.com/lowerquality/gentle) and [this work](https://ieeexplore.ieee.org/document/7404861) employ a weighted finite state transducer (WFST) framework to model the noisy texts. \n",
    "- [WhisperX](https://github.com/m-bain/whisperX) uses attention mechanism to propose rough time stamps for uniformly segmented audio. Then, it performs phone-level or word-level forced alignment with an external aligner.\n",
    "- [MMS](https://arxiv.org/abs/2305.13516) uses a special `<star>` token to handle missing words in the transcript.\n",
    "- [SailAlign](https://www.semanticscholar.org/paper/SailAlign%3A-Robust-long-speech-text-alignment-Katsamanis-Georgiou/0b7f86429641b188cc62ec32eee590e8795a3d02) iteratively identifies reliable regions and then narrows down to align the remaining unaligned regions.\n",
    "\n",
    "This tutorial is based on WFST and thus falls in the first category. Our implementation is based on PyTorch. Any CTC model in PyTorch can be equipped with our library to become a robust aligner. This makes our aligner distinguish from existing ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For WFST, our library depends on [k2](https://github.com/k2-fsa/k2/), a pytorch-based WFST library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check python and pytorch's version\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k2 (cpu)\n",
    "# pip install k2==1.24.4.dev20240223+cpu.torch2.2.1 -f https://k2-fsa.github.io/k2/cpu.html\n",
    "# k2 (gpu)\n",
    "!pip install k2==1.24.4.dev20240301+cuda12.1.torch2.2.1 -f https://k2-fsa.github.io/k2/cuda.html\n",
    "!pip install cmudict g2p_en\n",
    "!pip install git+https://github.com/huangruizhe/lis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone -b aligner_label_priors https://github.com/huangruizhe/audio.git\n",
    "# sys.path.append(\"audio\")\n",
    "\n",
    "sys.path.append(\"/exp/rhuang/meta/audio_latest/examples/asr/librispeech_alignment/alignment\")\n",
    "\n",
    "from torchaudio_k2_aligner import (\n",
    "    uniform_segmentation_with_overlap,\n",
    "    align_segments,\n",
    "    concat_alignments,\n",
    "    get_final_word_alignment,\n",
    "    align,\n",
    "    get_audacity_labels,\n",
    "    get_gentle_visualization,\n",
    ")\n",
    "from tokenizer import EnglishCharTokenizer\n",
    "from factor_transducer import make_factor_transducer_word_level_index_with_skip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pre-trained Wav2Vec2 model, [torchaudio.pipelines.MMS_FA](https://pytorch.org/audio/main/generated/torchaudio.pipelines.MMS_FA.html#torchaudio.pipelines.MMS_FA), as the acoustic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = torchaudio.pipelines.MMS_FA\n",
    "model = bundle.get_model(with_star=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('-', 'a', 'i', 'e', 'n', 'o', 'u', 't', 's', 'r', 'm', 'k', 'l', 'd', 'g', 'h', 'y', 'b', 'p', 'w', 'c', 'v', 'j', 'z', 'f', \"'\", 'q', 'x', '*')\n"
     ]
    }
   ],
   "source": [
    "LABELS = bundle.get_labels(star=\"*\")\n",
    "DICTIONARY = bundle.get_dict(star=\"*\")\n",
    "\n",
    "print(LABELS)\n",
    "\n",
    "tokenizer = EnglishCharTokenizer(\n",
    "    token2id=DICTIONARY,\n",
    "    blk_token=\"-\",\n",
    "    unk_token=\"*\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Prepare long audio and noisy text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its audiobook chapter (of 30 minutes) in the [LibriVox project](https://librivox.org/walden-by-henry-david-thoreau/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the whole book\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "text = soup.get_text()\n",
    "text = text.replace(\"\\r\\n\", \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a chapter of the audio book\n",
    "# !wget https://ia800707.us.archive.org/20/items/walden_librivox/walden_c07.mp3\n",
    "\n",
    "SPEECH_FILE = \"walden_c07.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play the long audio\n",
    "IPython.display.Audio(SPEECH_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transcript\n",
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the transcript relevant to the audio\n",
    "\n",
    "# In this running example, the whole audio corresponds to \n",
    "# text[271616: 293529]\n",
    "# or\n",
    "# text.split()[49489: 53362]\n",
    "\n",
    "print(text[271400: 271400+1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the audio contains a header \"This is a LibriVox recording ...\" which is not transcribed. On the other hand, as we have downloaded the whole book, it contains a lot of extra text that's not spoken in the audio. Obviously, the standard forced alignment algorithm will not work in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 118840 words in the text\n",
      "[['t', 'h', 'e'], ['b', 'e', 'a', 'n', 'f', 'i', 'e', 'l', 'd'], ['m', 'e', 'a', 'n', 'w', 'h', 'i', 'l', 'e'], ['m', 'y'], ['b', 'e', 'a', 'n', 's'], ['t', 'h', 'e'], ['l', 'e', 'n', 'g', 't', 'h'], ['o', 'f'], ['w', 'h', 'o', 's', 'e'], ['r', 'o', 'w', 's'], ['a', 'd', 'd', 'e', 'd'], ['t', 'o', 'g', 'e', 't', 'h', 'e', 'r'], ['w', 'a', 's'], ['s', 'e', 'v', 'e', 'n'], ['m', 'i', 'l', 'e', 's']]\n"
     ]
    }
   ],
   "source": [
    "# We tokenize the text into the labels of the acoustic model's output\n",
    "text_tokenized = tokenizer.encode(tokenizer.text_normalize(text))\n",
    "\n",
    "print(f\"There are {len(text_tokenized)} words in the text\")\n",
    "\n",
    "# Preview the tokenization results. This corresponds to the beginning of the audio\n",
    "print(tokenizer.decode(text_tokenized)[49489: 49489+15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Use WFST to represent the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 WFST basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 CTC graph, factor transducer and the variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will represent the whole book of over 100K words into one single WFST decoding graph that allows occasional insertion/deletioin/substitution errors. We set `skip_penalty=-0.5` and `return_penalty=-18.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1028287 nodes and 3544014 arcs in the decoding graph for the text of 118840 words.\n",
      "The decoding graph is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "decoding_graph, word_index_sym_tab, token_sym_tab = \\\n",
    "    make_factor_transducer_word_level_index_with_skip(\n",
    "        text_tokenized, \n",
    "        blank_penalty=0, \n",
    "        skip_penalty=-0.5, \n",
    "        return_penalty=-18.0\n",
    "    )\n",
    "decoding_graph = decoding_graph.to(device)\n",
    "\n",
    "print(f\"There are {decoding_graph.shape[0]} nodes and {decoding_graph.num_arcs} arcs in the decoding graph for the text of {len(text_tokenized)} words.\")\n",
    "print(f\"The decoding graph is on device: {decoding_graph.device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle long audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hope to feed the audio into the neural network to get the frame-wise posteriors (i.e., the emission matrix) over the label vocabulary. However, the audio book chapter is of about 30 minutes. This is too long to feed into the acoustic model all at once. \n",
    "\n",
    "A common practice is to segment the long audio into small overlapping segments. The segments are processed independently and the results are concatenated appropriately to make the final alignment result. Here, as the Wav2Vec2 pretrained model that we use is taking a raw wave form as input, so we will segment the original audio into 15-second segments. In practice, we can also segment the feature vector (e.g., Fbanks) to feed into the acoustic model.\n",
    "\n",
    "The library that we provide in this tutorial provides functions for appropriate segmentation and concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the library to segment the 30-minute audio into 15-second segments, with an overlap of 2 seconds between neighboring segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 29075748]) 16000\n"
     ]
    }
   ],
   "source": [
    "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
    "resample_rate = 16000  # this is the sample rate of the Wav2Vec2 model\n",
    "waveform = torchaudio.functional.resample(waveform, sample_rate, resample_rate)\n",
    "sample_rate = resample_rate\n",
    "print(waveform.shape, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([140, 240128]) torch.Size([140]) torch.Size([140])\n"
     ]
    }
   ],
   "source": [
    "if waveform.dim() == 2:\n",
    "    waveform.unsqueeze_(-1)\n",
    "segment_size = sample_rate * 15 + 128  # 15 seconds; use extra 128 waveform samples to make sure we have 750 frames for each full-sized segment\n",
    "overlap = sample_rate * 2 + 128        # 2 seconds\n",
    "shortest_segment_size = sample_rate * 0.2  # if the last segment has less than 3200 samples (0.2 seconds), it will be discarded\n",
    "\n",
    "waveform_segmented, segment_lengths, segment_offsets = uniform_segmentation_with_overlap(\n",
    "    waveform,\n",
    "    segment_size, \n",
    "    overlap, \n",
    "    shortest_segment_size=shortest_segment_size\n",
    ")\n",
    "waveform_segmented = waveform_segmented.squeeze()\n",
    "print(waveform_segmented.shape, segment_lengths.shape, segment_offsets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can listen to a segment to make sure the segmentation pipeline runs correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(waveform_segmented[0], rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Obtain alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will feed the short segments to the neural network based acoustic model. We do this in batches. For each segment, the acoustic model produces frame-wise classification `batch_emissions` over its label vocabulary. This is then combined with the WFST `decoding_graph`, which is shared by all segments, to produce the best alignment path for each segment. If the `decoding_graph` is a linear WFST, which means there is no insertion/deletion/substitution errors in the transcript, this is equivalent to the conventional forced alignment as provided by this [TorchAudio API](https://pytorch.org/audio/main/tutorials/ctc_forced_alignment_api_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There something to note for the `decoding_graph`. When we combine it with the neural network output, its \"input label\" should match the network's label vocabulary. However, it is our decision to define decoding graph's \"output labels\", which basically ask this question: what do we align the audio to? Here are two examples of design choices:\n",
    "\n",
    "- The output labels are word labels or phoneme labels, as in most WFST-based alignment solutions (e.g., in [Gentle](https://github.com/lowerquality/gentle/blob/master/gentle/diff_align.py#L16)). In this case, the alignment paths consist of the word-level or phoneme-level transcripts predicted by the model.\n",
    "\n",
    "- The output labels are the **word indices** in the transcript, instead of word labels. There are two benefits: (1) we can obtain the word labels easily from the word indices; (2) with word indices, we are able to preserve the ordering of words in the transcript, even though the alignment paths may contain words in the order different from the original transcript. As we will see later, this word ordering information enables efficient and effective heuristics for post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:22<00:00,  4.53s/it]\n"
     ]
    }
   ],
   "source": [
    "if device == torch.device(\"cpu\"):\n",
    "    batch_size = 2\n",
    "else:\n",
    "    batch_size = 32\n",
    "\n",
    "output_frames_offset = segment_offsets // (sample_rate * 0.02)\n",
    "\n",
    "alignment_results = list()\n",
    "for i in tqdm(range(0, waveform_segmented.size(0), batch_size)):\n",
    "    batch_waveform_segmented = waveform_segmented[i: i+batch_size].to(device)\n",
    "    batch_segment_lengths = segment_lengths[i: i+batch_size]\n",
    "    batch_output_frames_offset = output_frames_offset[i: i+batch_size]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        # Checkout the API of the forward function here: https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/utils.py#L34\n",
    "        batch_emissions, batch_emissions_lengths = model(batch_waveform_segmented.to(device), batch_segment_lengths.to(device))\n",
    "    \n",
    "    # Attach the star dimension manually, see torchaudio issue #3772\n",
    "    star_dim = torch.empty((batch_emissions.size(0), batch_emissions.size(1), 1), device=batch_emissions.device, dtype=batch_emissions.dtype)\n",
    "    star_dim[:] = -5.0\n",
    "    batch_emissions = torch.cat((batch_emissions, star_dim), 2)\n",
    "\n",
    "    # `token_ids` and `timestamps` will each be a list of lists.\n",
    "    # Each sublist corresponds to a segment in the batch.\n",
    "    batch_results = align_segments(\n",
    "        batch_emissions,\n",
    "        decoding_graph,\n",
    "        batch_emissions_lengths,\n",
    "    )\n",
    "\n",
    "    # The interpretation of `token.token_id` depends on the decoding graph.\n",
    "    # Here, in this tutorial, `token.token_id` is the key to the `word_index_sym_tab``\n",
    "    # and `token_sym_tab` dictionaries.\n",
    "    for aligned_tokens, offset in zip(batch_results, batch_output_frames_offset):\n",
    "        for token in aligned_tokens:\n",
    "            token.timestamp += offset  # This will become the absolute frame timestamp in the whole audio\n",
    "            if token.token_id == tokenizer.blk_id:\n",
    "                continue\n",
    "            if token.token_id in word_index_sym_tab:\n",
    "                token.attr[\"wid\"] = word_index_sym_tab[token.token_id]\n",
    "            if token.token_id in token_sym_tab:\n",
    "                token.attr[\"tk\"] = token_sym_tab[token.token_id]            \n",
    "\n",
    "    alignment_results.extend(batch_results)\n",
    "\n",
    "    # It will be too slow to do inference on CPU. \n",
    "    # As it's only for the demonstration purpose, so we will break the loop here.\n",
    "    # We can still see some partial alignment results.\n",
    "    if device == torch.device(\"cpu\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got the alignment results for all 140 15-second segments in less than a minute, thanks to GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alignment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to concatenate the alignment results appropriately. We need to take care of two things: (1) the mis-aligned results, (2) the overlapping parts. The common algorithm for this concatenation step is through [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) (e.g., in [Gentle](https://github.com/lowerquality/gentle/blob/master/gentle/diff_align.py#L16)), which aligns the forced-aligned transcript with the ground-truth transcript. As we have word indices instead of word labels, we will do this a bit differently. Given the alignment results for all segments, we will find the [longest increasing subsequence (LIS)](https://en.wikipedia.org/wiki/Longest_increasing_subsequence) in the word indices. This can be done in $O(NlogN)$ time complexity and $O(N)$ space complexity, instead of $O(N^2)$ by Levenshtein distance, where $N$ is the total length of the segment-wise alignment results. This can be useful especially when $N$ is large for long audios.\n",
    "\n",
    "Moreover, with word indices, we are able to develop heuristics to remove outliers in the alignment results. This has been all implemented in our alignment library. E.g., by calling the `concat_alignments` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16163, 41)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: \n",
    "# The beginning and ending of the audio is hard to be aligned to the book.\n",
    "# We might need to use VAD or something to handle it. Or predefine the start/end of the audio and text.\n",
    "# Ignored them for now cos we just use the alignment for ASR training.\n",
    "\n",
    "# `resolved_alignment_results` is a list of `AlignedToken`\n",
    "# `unaligned_text_indices` is a list of (start_word_index, end_word_index)\n",
    "#    which corresponds to \"holes\" in the long text that are not aligned\n",
    "resolved_alignment_results, unaligned_text_indices = concat_alignments(\n",
    "    alignment_results, \n",
    "    neighborhood_size=5,\n",
    ")\n",
    "\n",
    "len(resolved_alignment_results), len(unaligned_text_indices)\n",
    "\n",
    "# text_splitted = text.split()\n",
    "# [text_splitted[tk.attr['wid']] if 'wid' in tk.attr else tokenizer.id2token[tk.attr['tk']] for tk in resolved_alignment_results[-40:-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final word-level alignment results can be obtained by the following, where `word_alignment` is a dictionary from word indices in `text` to an `AlignedWord` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `word_alignment` is a dict of word index in the long text => AlignedWord object\n",
    "word_alignment = get_final_word_alignment(resolved_alignment_results, text, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's preview the alignment results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(49491,\n",
       "  AlignedWord(word='Meanwhile', start_time=tensor(1256.), end_time=None, phones=[AlignedToken(token_id='m', timestamp=tensor(1256.), attr=None), AlignedToken(token_id='e', timestamp=tensor(1258.), attr=None), AlignedToken(token_id='a', timestamp=tensor(1261.), attr=None), AlignedToken(token_id='n', timestamp=tensor(1263.), attr=None), AlignedToken(token_id='w', timestamp=tensor(1266.), attr=None), AlignedToken(token_id='h', timestamp=tensor(1267.), attr=None), AlignedToken(token_id='h', timestamp=tensor(1268.), attr=None), AlignedToken(token_id='i', timestamp=tensor(1274.), attr=None), AlignedToken(token_id='l', timestamp=tensor(1278.), attr=None), AlignedToken(token_id='e', timestamp=tensor(1280.), attr=None)])),\n",
       " (49492,\n",
       "  AlignedWord(word='my', start_time=tensor(1283.), end_time=None, phones=[AlignedToken(token_id='m', timestamp=tensor(1283.), attr=None), AlignedToken(token_id='y', timestamp=tensor(1296.), attr=None)])),\n",
       " (49493,\n",
       "  AlignedWord(word='beans,', start_time=tensor(1303.), end_time=None, phones=[AlignedToken(token_id='b', timestamp=tensor(1303.), attr=None), AlignedToken(token_id='e', timestamp=tensor(1307.), attr=None), AlignedToken(token_id='n', timestamp=tensor(1318.), attr=None), AlignedToken(token_id='s', timestamp=tensor(1329.), attr=None)])),\n",
       " (49494,\n",
       "  AlignedWord(word='the', start_time=tensor(1351.), end_time=None, phones=[AlignedToken(token_id='t', timestamp=tensor(1351.), attr=None), AlignedToken(token_id='h', timestamp=tensor(1352.), attr=None), AlignedToken(token_id='e', timestamp=tensor(1354.), attr=None)])),\n",
       " (49495,\n",
       "  AlignedWord(word='length', start_time=tensor(1358.), end_time=None, phones=[AlignedToken(token_id='l', timestamp=tensor(1358.), attr=None), AlignedToken(token_id='e', timestamp=tensor(1361.), attr=None), AlignedToken(token_id='n', timestamp=tensor(1364.), attr=None), AlignedToken(token_id='g', timestamp=tensor(1366.), attr=None), AlignedToken(token_id='t', timestamp=tensor(1368.), attr=None), AlignedToken(token_id='h', timestamp=tensor(1369.), attr=None)])),\n",
       " (49496,\n",
       "  AlignedWord(word='of', start_time=tensor(1372.), end_time=None, phones=[AlignedToken(token_id='o', timestamp=tensor(1372.), attr=None), AlignedToken(token_id='f', timestamp=tensor(1374.), attr=None)])),\n",
       " (49497,\n",
       "  AlignedWord(word='whose', start_time=tensor(1377.), end_time=None, phones=[AlignedToken(token_id='w', timestamp=tensor(1377.), attr=None)])),\n",
       " (49498,\n",
       "  AlignedWord(word='rows,', start_time=tensor(1392.), end_time=None, phones=[AlignedToken(token_id='r', timestamp=tensor(1392.), attr=None), AlignedToken(token_id='o', timestamp=tensor(1395.), attr=None), AlignedToken(token_id='s', timestamp=tensor(1398.), attr=None)])),\n",
       " (49499,\n",
       "  AlignedWord(word='added', start_time=tensor(1399.), end_time=None, phones=[AlignedToken(token_id='a', timestamp=tensor(1399.), attr=None)])),\n",
       " (49500,\n",
       "  AlignedWord(word='together,', start_time=tensor(1425.), end_time=None, phones=[AlignedToken(token_id='t', timestamp=tensor(1425.), attr=None), AlignedToken(token_id='o', timestamp=tensor(1427.), attr=None), AlignedToken(token_id='g', timestamp=tensor(1431.), attr=None), AlignedToken(token_id='e', timestamp=tensor(1434.), attr=None), AlignedToken(token_id='t', timestamp=tensor(1439.), attr=None), AlignedToken(token_id='h', timestamp=tensor(1440.), attr=None), AlignedToken(token_id='e', timestamp=tensor(1441.), attr=None), AlignedToken(token_id='r', timestamp=tensor(1443.), attr=None)]))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_alignment.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(53352,\n",
       "  AlignedWord(word='mind', start_time=tensor(90157.), end_time=None, phones=[AlignedToken(token_id='m', timestamp=tensor(90157.), attr=None), AlignedToken(token_id='i', timestamp=tensor(90168.), attr=None), AlignedToken(token_id='n', timestamp=tensor(90171.), attr=None), AlignedToken(token_id='n', timestamp=tensor(90172.), attr=None), AlignedToken(token_id='d', timestamp=tensor(90175.), attr=None)])),\n",
       " (53353,\n",
       "  AlignedWord(word='not', start_time=tensor(90212.), end_time=None, phones=[AlignedToken(token_id='n', timestamp=tensor(90212.), attr=None)])),\n",
       " (53354,\n",
       "  AlignedWord(word='only', start_time=tensor(90224.), end_time=None, phones=[AlignedToken(token_id='o', timestamp=tensor(90224.), attr=None), AlignedToken(token_id='n', timestamp=tensor(90227.), attr=None), AlignedToken(token_id='l', timestamp=tensor(90229.), attr=None), AlignedToken(token_id='l', timestamp=tensor(90230.), attr=None), AlignedToken(token_id='y', timestamp=tensor(90231.), attr=None)])),\n",
       " (53355,\n",
       "  AlignedWord(word='his', start_time=tensor(90236.), end_time=None, phones=[AlignedToken(token_id='h', timestamp=tensor(90236.), attr=None)])),\n",
       " (53356,\n",
       "  AlignedWord(word='first', start_time=tensor(90253.), end_time=None, phones=[AlignedToken(token_id='f', timestamp=tensor(90253.), attr=None), AlignedToken(token_id='i', timestamp=tensor(90258.), attr=None), AlignedToken(token_id='r', timestamp=tensor(90263.), attr=None), AlignedToken(token_id='s', timestamp=tensor(90271.), attr=None), AlignedToken(token_id='t', timestamp=tensor(90276.), attr=None)])),\n",
       " (53357,\n",
       "  AlignedWord(word='but', start_time=tensor(90318.), end_time=None, phones=[AlignedToken(token_id='b', timestamp=tensor(90318.), attr=None), AlignedToken(token_id='u', timestamp=tensor(90320.), attr=None), AlignedToken(token_id='t', timestamp=tensor(90323.), attr=None)])),\n",
       " (53358,\n",
       "  AlignedWord(word='his', start_time=tensor(90326.), end_time=None, phones=[AlignedToken(token_id='h', timestamp=tensor(90326.), attr=None), AlignedToken(token_id='i', timestamp=tensor(90328.), attr=None), AlignedToken(token_id='s', timestamp=tensor(90331.), attr=None)])),\n",
       " (53359,\n",
       "  AlignedWord(word='last', start_time=tensor(90338.), end_time=None, phones=[AlignedToken(token_id='l', timestamp=tensor(90338.), attr=None), AlignedToken(token_id='a', timestamp=tensor(90343.), attr=None), AlignedToken(token_id='s', timestamp=tensor(90352.), attr=None), AlignedToken(token_id='s', timestamp=tensor(90353.), attr=None), AlignedToken(token_id='t', timestamp=tensor(90357.), attr=None)])),\n",
       " (53360,\n",
       "  AlignedWord(word='fruits', start_time=tensor(90363.), end_time=None, phones=[AlignedToken(token_id='f', timestamp=tensor(90363.), attr=None), AlignedToken(token_id='r', timestamp=tensor(90367.), attr=None), AlignedToken(token_id='u', timestamp=tensor(90370.), attr=None), AlignedToken(token_id='i', timestamp=tensor(90373.), attr=None), AlignedToken(token_id='i', timestamp=tensor(90374.), attr=None), AlignedToken(token_id='t', timestamp=tensor(90377.), attr=None), AlignedToken(token_id='s', timestamp=tensor(90383.), attr=None)])),\n",
       " (53361,\n",
       "  AlignedWord(word='also.', start_time=tensor(90411.), end_time=None, phones=[AlignedToken(token_id='a', timestamp=tensor(90411.), attr=None), AlignedToken(token_id='l', timestamp=tensor(90416.), attr=None), AlignedToken(token_id='s', timestamp=tensor(90422.), attr=None), AlignedToken(token_id='o', timestamp=tensor(90426.), attr=None)]))]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(word_alignment.items())[-10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, the alignment results look pretty good! Despite the book being long and noisy (e.g., ), it successfully locates [the chapter](https://www.gutenberg.org/cache/epub/205/pg205-images.html#chap08) from the whole book corresponding to the audio, except the first two words (\"The Bean-Field\") which are the chapter title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `unaligned_text_indices`, we can see the parts in the book chapter that are not successfully aligned to the audio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 18 holes of size greater than one word, out of 41 holes in total.\n",
      "\n",
      "Unaligned text range: [49633, 49637]\n",
      "Unaligned text: ['cherish', 'them,', 'I', 'hoe', 'them,']\n",
      "Time range: 92.88 ~ 97.24 seconds\n",
      "\n",
      "Unaligned text range: [50262, 50263]\n",
      "Unaligned text: ['very', 'agricola']\n",
      "Time range: 364.94 ~ 368.42 seconds\n",
      "\n",
      "Unaligned text range: [50627, 50633]\n",
      "Unaligned text: ['I', 'cultivated,', 'and', 'my', 'hoe', 'played', 'the']\n",
      "Time range: 534.18 ~ 539.00 seconds\n",
      "\n",
      "Unaligned text range: [50929, 50954]\n",
      "Unaligned text: ['day', 'of', 'it—like', 'a', 'mote', 'in', 'the', 'eye,', 'or', 'in', 'heaven’s', 'eye,', 'falling', 'from', 'time', 'to', 'time', 'with', 'a', 'swoop', 'and', 'a', 'sound', 'as', 'if', 'the']\n",
      "Time range: 664.98 ~ 675.66 seconds\n",
      "\n",
      "Unaligned text range: [51193, 51199]\n",
      "Unaligned text: ['far.', 'To', 'me,', 'away', 'there', 'in', 'my']\n",
      "Time range: 794.90 ~ 799.00 seconds\n",
      "\n",
      "Unaligned text range: [51509, 51512]\n",
      "Unaligned text: ['my', 'chivalry', 'upon.', 'These']\n",
      "Time range: 924.86 ~ 928.34 seconds\n",
      "\n",
      "Unaligned text range: [51971, 51975]\n",
      "Unaligned text: ['as', 'Evelyn', 'says,', '“no', 'compost']\n",
      "Time range: 1132.18 ~ 1135.30 seconds\n",
      "\n",
      "Unaligned text range: [51994, 51999]\n",
      "Unaligned text: ['earth,”', 'he', 'adds', 'elsewhere,', '“especially', 'if']\n",
      "Time range: 1144.54 ~ 1149.88 seconds\n",
      "\n",
      "Unaligned text range: [52116, 52117]\n",
      "Unaligned text: ['$', '0.54']\n",
      "Time range: 1205.00 ~ 1208.24 seconds\n",
      "\n",
      "Unaligned text range: [52123, 52132]\n",
      "Unaligned text: ['Too', 'much.', 'Beans', 'for', 'seed,..............................', '3.12½', 'Potatoes', 'for', 'seed,...........................', '1.33']\n",
      "Time range: 1209.88 ~ 1224.68 seconds\n",
      "\n",
      "Unaligned text range: [52136, 52139]\n",
      "Unaligned text: ['0.40', 'Turnip', 'seed,.................................', '0.06']\n",
      "Time range: 1225.56 ~ 1231.54 seconds\n",
      "\n",
      "Unaligned text range: [52159, 52160]\n",
      "Unaligned text: ['0.75', '————']\n",
      "Time range: 1242.76 ~ 1246.94 seconds\n",
      "\n",
      "Unaligned text range: [52190, 52197]\n",
      "Unaligned text: ['\"', 'small,.............................', '2.25', 'Grass,..........................................', '1.00', 'Stalks,.........................................', '0.75', '————']\n",
      "Time range: 1279.74 ~ 1286.70 seconds\n",
      "\n",
      "Unaligned text range: [52201, 52207]\n",
      "Unaligned text: ['Leaving', 'a', 'pecuniary', 'profit,', 'as', 'I', 'have']\n",
      "Time range: 1287.16 ~ 1293.02 seconds\n",
      "\n",
      "Unaligned text range: [52252, 52253]\n",
      "Unaligned text: ['look', 'out']\n",
      "Time range: 1313.84 ~ 1315.04 seconds\n",
      "\n",
      "Unaligned text range: [52416, 52421]\n",
      "Unaligned text: ['I', 'said', 'this', 'to', 'myself;', 'but']\n",
      "Time range: 1379.50 ~ 1384.24 seconds\n",
      "\n",
      "Unaligned text range: [53045, 53053]\n",
      "Unaligned text: ['are', 'particularly', 'pious', 'or', 'just,', '(maximeque', 'pius', 'quæstus),', 'and']\n",
      "Time range: 1652.52 ~ 1661.82 seconds\n",
      "\n",
      "Unaligned text range: [53064, 53070]\n",
      "Unaligned text: ['Mother', 'and', 'Ceres,', 'and', 'thought', 'that', 'they']\n",
      "Time range: 1665.84 ~ 1669.50 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The range [s, e] (both inclusive) is a \"hole\" in the long text that is not aligned\n",
    "# where s and e are the word indices in the long text\n",
    "text_splitted = text.split()\n",
    "print(f\"There are {len([None for s, e in unaligned_text_indices if e - s > 0])} holes of size greater than one word, out of {len(unaligned_text_indices)} holes in total.\\n\")\n",
    "for s, e in unaligned_text_indices:\n",
    "    if e - s > 0: # ignore the \"holes\" of only one word\n",
    "        print(f\"Unaligned text range: [{s}, {e}]\")\n",
    "        print(f\"Unaligned text: {text_splitted[s:e+1]}\")\n",
    "        print(f\"Time range: {word_alignment[s-1].start_time * 0.02:.2f} ~ {word_alignment[e+1].start_time * 0.02:.2f} seconds\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can listen to and visually check the alignment results. We will provide two methods, [Audacity](https://www.audacityteam.org/) and [Gentle](http://lowerquality.com/gentle/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Audacity, we will provide the label file that can be imported to the software along with the audio file. Follow the instructions [here](https://manual.audacityteam.org/man/importing_and_exporting_labels.html) to import labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "audacity_labels_str = get_audacity_labels(word_alignment, frame_duration=0.02)\n",
    "\n",
    "audacity_path = str(SPEECH_FILE)[:-4] + \"_audacity.txt\"\n",
    "with open(audacity_path, \"w\") as fout:\n",
    "    print(audacity_labels_str, file=fout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Gentle, it comes with a handly [visualization mechanism](https://github.com/lowerquality/gentle/blob/master/serve.py#L113) based on HTML and JavaScript. The following function will generate the necessary files (the \"index.html\" file) and instructions for visualization in Gentle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gentle visualization is saved to: /exp/rhuang/meta/audio_latest/examples/asr/librispeech_alignment/alignment/index.html\n",
      "Usage instrunctions:\n",
      "- Download both the audio file and the index.html file to the same folder in your local machine;\n",
      "- Open the index.html file in a browser.\n"
     ]
    }
   ],
   "source": [
    "get_gentle_visualization(word_alignment, tokenizer, frame_duration=0.02, audio_file=SPEECH_FILE, text=text, i_word_start=49489, i_word_end=53362)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- dataclass for alignment results [done]\n",
    "- inspect the improvement for walden\n",
    "- phone-level alignment [done]\n",
    "- gentle [done]\n",
    "- quasi-gentle in colab\n",
    "- training recipe: need to start it tonight [done]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio_k2_aligner\n",
    "\n",
    "import importlib\n",
    "importlib.reload(torchaudio_k2_aligner)\n",
    "\n",
    "get_gentle_visualization = torchaudio_k2_aligner.get_gentle_visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
