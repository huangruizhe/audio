{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcU8WMfnqm2w"
      },
      "source": [
        "# A tutorial on obtaining accurate speech-to-text alignment for long audio and noisy text for 1100+ languages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5V6Omjoq0Cl"
      },
      "source": [
        "This tutorial consists of two parts.\n",
        "- The first part corresponds to the paper *Less Peaky And More Accurate CTC Forced Alignment by Label Priors* published in ICASSP 2024. We will desmonstrate how to obtain more accurate speech-to-text alignment compared to a standard CTC model.\n",
        "- In the second part, we will provide a robust pytorch-based speech-to-text alignment library (\"TorchAudio Aligner\") to align long audio and noisy text. We will replicate the data preparation step for preparing the [Librispeech corpus](https://www.openslr.org/12). For example, aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its [audiobook chapter](https://librivox.org/walden-by-henry-david-thoreau/) (of 30 minutes in this demo, or even longer) in the [LibriVox project](https://librivox.org/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uqbsHWYWP7g"
      },
      "source": [
        "20240417: This tutorial is still under development. This is the github repository: [https://github.com/huangruizhe/audio/tree/aligner_label_priors/examples/asr/librispeech_alignment](https://github.com/huangruizhe/audio/tree/aligner_label_priors/examples/asr/librispeech_alignment).\n",
        "\n",
        "If you have any questions, please feel free to contact me (huangruizhe09@gmail.com)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A84cyisurZor"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGHi_TOZkNRL",
        "outputId": "c396ec5f-7897-4671-e1ce-1ae8de2df1d2"
      },
      "outputs": [],
      "source": [
        "# Check python and pytorch's version\n",
        "!python --version\n",
        "\n",
        "# import necessary libraries\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"TorchAudio: {torchaudio.__version__}\")\n",
        "print(f\"CUDA: {torch.version.cuda}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "import IPython\n",
        "import sys\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xc0gUhOKqSX"
      },
      "source": [
        "For weighted finite state transducer (WFST), our library depends on [k2](https://github.com/k2-fsa/k2/), a pytorch-based WFST library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za3gKTNuKsPZ",
        "outputId": "cd0aafe7-9f16-4b92-b7b5-cc29d88b3cec"
      },
      "outputs": [],
      "source": [
        "# This will take approximately 1~2 minutes\n",
        "\n",
        "# k2 (cpu)\n",
        "# pip install k2==1.24.4.dev20240223+cpu.torch2.2.1 -f https://k2-fsa.github.io/k2/cpu.html\n",
        "# k2 (gpu)\n",
        "!pip install k2==1.24.4.dev20240301+cuda12.1.torch2.2.1 -f https://k2-fsa.github.io/k2/cuda.html\n",
        "!pip install pytorch-lightning\n",
        "!pip install cmudict g2p_en\n",
        "!pip install pydub\n",
        "!pip install git+https://github.com/huangruizhe/lis.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txG9BjFKKvgc",
        "outputId": "cbaa70a3-9d6b-435a-8048-33c214440d45"
      },
      "outputs": [],
      "source": [
        "![ ! -d \"audio\" ] && git clone -b aligner_label_priors https://github.com/huangruizhe/audio.git\n",
        "sys.path.append(\"audio/examples/asr/librispeech_alignment\")\n",
        "sys.path.append(\"audio/examples/asr/librispeech_alignment/alignment\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz6c_WIVr25X"
      },
      "source": [
        "## Part 1: obtaining more accurate CTC alignment by label priors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMkTOE_KQ_a6"
      },
      "source": [
        "If you are not interested in Part 1, just go straight to Part 2. We have made the codes in Part 2 independent of Part 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzMdOJ3jr5BU"
      },
      "source": [
        "### 1.1 Alignment with standard CTC model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lcn0gNFK7H1"
      },
      "source": [
        "#### 1.1.1 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH6W6sIZK-_d"
      },
      "source": [
        "The standard CTC model can be trained with the following command. As in the paper, the acoustic model is a TDNN-FFN model of 5M parameters. The frame rate is 0.02 seconds.\n",
        "```\n",
        "cd audio/examples/asr/librispeech_alignment/\n",
        "exp_dir=\"experiments/ctc_tdnndnn_k2_0.0\"\n",
        "\n",
        "python train.py \\\n",
        "--exp-dir $exp_dir \\   \n",
        "--librispeech-path <path-to-librispeech> \\\n",
        "--global-stats-path ./global_stats.json \\\n",
        "--epochs 10 \\\n",
        "--nodes 1 \\\n",
        "--gpus 4\n",
        "```\n",
        "\n",
        "This will train a model in 10 epochs, with train/validation loss going from 3053/2394 down to 1990/1549."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRVHOSIwLBuZ"
      },
      "source": [
        "#### 1.1.2 Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDh3Oz1rLEZ-"
      },
      "source": [
        "The CTC model can be decoded with the following command. Note, the main purpose of the model is not for speech recognition. We just decode it with greedy algorithm as a sanity check. The character error rates (CER) on Librispeech are 4.98%/5.61% on test-clean and test-other.\n",
        "\n",
        "```\n",
        "python eval.py \\\n",
        "--checkpoint-path $exp_dir/checkpoints/epoch=9-step=65369.ckpt \\\n",
        "--librispeech-path <path-to-librispeech> \\\n",
        "--global-stats-path ./global_stats.json  \\\n",
        "--use-cuda\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-mKcTZ3LHtY"
      },
      "source": [
        "#### 1.1.3 Alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvFdfimQLJtW",
        "outputId": "0bb48687-6f5c-4659-ad27-4b4eb9a2c392"
      },
      "outputs": [],
      "source": [
        "SPEECH_FILE = torchaudio.utils.download_asset(\"tutorial-assets/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\")\n",
        "transcript = \"i had that curiosity beside me at this moment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "yYJ26YjMLLuN",
        "outputId": "fdaf9e18-3845-4515-a4d7-27b384f81b1c"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(SPEECH_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4ye0ktDLNwc"
      },
      "outputs": [],
      "source": [
        "from alignment.tokenizer import EnglishPhonemeTokenizer\n",
        "from transforms import TestTransform\n",
        "from lightning import AcousticModelModule\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "matplotlib.rcParams[\"figure.figsize\"] = [16.0, 3.0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2XLfcL_LpaC"
      },
      "source": [
        "Prepare data for the acoustic model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eQ_sHkGLq4_",
        "outputId": "cdbc4f7c-53bd-4ade-f346-1ff12173f1a5"
      },
      "outputs": [],
      "source": [
        "tokenizer = EnglishPhonemeTokenizer()\n",
        "global_stats_path = \"audio/examples/asr/librispeech_alignment/global_stats.json\"\n",
        "test_transform = TestTransform(global_stats_path=global_stats_path, tokenizer=tokenizer)\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
        "speaker_id, chapter_id, utterance_id = 0, 0, 0\n",
        "\n",
        "sample = (waveform, sample_rate, transcript, speaker_id, chapter_id, utterance_id)\n",
        "batch, samples = test_transform(sample)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfScpl5WLxEo"
      },
      "source": [
        "Load the acoustic model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkeY6mkLLy2R",
        "outputId": "5c5b9d0d-2969-4a00-fd1d-97c05fbc1b19"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"audio/examples/asr/librispeech_alignment/alignment/checkpoints/ctc_tdnndnn_k2_0.0/epoch=9-step=65369.ckpt\"\n",
        "model_standard = AcousticModelModule.load_from_checkpoint(checkpoint_path, tokenizer=tokenizer).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2gXMGnLMHyV"
      },
      "source": [
        "Get the forced alignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGegaq0jMHgc"
      },
      "outputs": [],
      "source": [
        "def unflatten(list_, lengths):\n",
        "    assert len(list_) == sum(lengths)\n",
        "    i = 0\n",
        "    ret = []\n",
        "    for l in lengths:\n",
        "        ret.append(list_[i : i + l])\n",
        "        i += l\n",
        "    return ret\n",
        "\n",
        "tokenized_transcript = tokenizer.encode_flatten(transcript, out_type=int)\n",
        "emission1 = model_standard.forward(batch)\n",
        "aligned_tokens1, alignment_scores1 = model_standard.align(batch, tokenized_transcript)\n",
        "token_spans1 = torchaudio.functional.merge_tokens(aligned_tokens1, alignment_scores1)\n",
        "word_spans1 = unflatten(token_spans1, [len(tokenizer.encode_flatten(word)) for word in transcript.split()])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmUfr3FTMrl7"
      },
      "source": [
        "Visualize the alignment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Kdxj2l_oMtYX",
        "outputId": "9e9b3895-891c-4cd1-a15c-97195b23f485"
      },
      "outputs": [],
      "source": [
        "# Compute average score weighted by the span length\n",
        "def _score(spans):\n",
        "    return sum(s.score * len(s) for s in spans) / sum(len(s) for s in spans)\n",
        "\n",
        "\n",
        "def plot_alignments(waveform, token_spans, emission, transcript, sample_rate=0.02):\n",
        "    ratio = waveform.size(1) / emission.size(1) / sample_rate\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1)\n",
        "    axes[0].imshow(emission[0].detach().cpu().T, aspect=\"auto\")\n",
        "    axes[0].set_title(\"Emission\")\n",
        "    axes[0].set_xticks([])\n",
        "\n",
        "    axes[1].specgram(waveform[0], Fs=sample_rate)\n",
        "    for t_spans, chars in zip(token_spans, transcript):\n",
        "        t0, t1 = t_spans[0].start + 0.1, t_spans[-1].end - 0.1\n",
        "        axes[0].axvspan(t0 - 0.5, t1 - 0.5, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
        "        axes[1].axvspan(ratio * t0, ratio * t1, facecolor=\"None\", hatch=\"/\", edgecolor=\"white\")\n",
        "        axes[1].annotate(f\"{_score(t_spans):.2f}\", (ratio * t0, sample_rate * 0.51), annotation_clip=False)\n",
        "\n",
        "        for span, char in zip(t_spans, chars):\n",
        "            t0 = span.start * ratio\n",
        "            axes[1].annotate(char, (t0, sample_rate * 0.55), annotation_clip=False)\n",
        "\n",
        "    axes[1].set_xlabel(\"time [second]\")\n",
        "    axes[1].set_xlim([0, None])\n",
        "    fig.tight_layout()\n",
        "\n",
        "\n",
        "plot_alignments(waveform, word_spans1, emission1, transcript.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "fqNN08EYMvsZ",
        "outputId": "a511ab57-2ace-4b1e-9eb8-6d3ad0df0188"
      },
      "outputs": [],
      "source": [
        "def plot_scores(word_spans, scores):\n",
        "    fig, ax = plt.subplots()\n",
        "    span_xs, span_hs = [], []\n",
        "    ax.axvspan(word_spans[0][0].start - 0.05, word_spans[-1][-1].end + 0.05, facecolor=\"paleturquoise\", edgecolor=\"none\", zorder=-1)\n",
        "    for t_span in word_spans:\n",
        "        for span in t_span:\n",
        "            for t in range(span.start, span.end):\n",
        "                span_xs.append(t + 0.5)\n",
        "                span_hs.append(scores[t].item())\n",
        "            ax.annotate(tokenizer.id2token[span.token], (span.start, -0.07))\n",
        "        ax.axvspan(t_span[0].start - 0.05, t_span[-1].end + 0.05, facecolor=\"mistyrose\", edgecolor=\"none\", zorder=-1)\n",
        "    ax.bar(span_xs, span_hs, color=\"lightsalmon\", edgecolor=\"coral\")\n",
        "    ax.set_title(\"Frame-level scores and word segments\")\n",
        "    ax.set_ylim(-0.1, None)\n",
        "    ax.grid(True, axis=\"y\")\n",
        "    ax.axhline(0, color=\"black\")\n",
        "    fig.tight_layout()\n",
        "\n",
        "\n",
        "plot_scores(word_spans1, alignment_scores1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESTU39LXr7CR"
      },
      "source": [
        "### 1.2 Alignment with CTC model with label priors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpTyZjoSNGYS"
      },
      "source": [
        "The CTC model with label priors can be trained with the following command. The only difference is that we added the \"--alpha 0.6\" option.\n",
        "```\n",
        "cd audio/examples/asr/librispeech_alignment/\n",
        "exp_dir=\"experiments/ctc_tdnndnn_k2_0.6\"\n",
        "\n",
        "python train.py \\\n",
        "--exp-dir $exp_dir \\   \n",
        "--librispeech-path <path-to-librispeech> \\\n",
        "--global-stats-path ./global_stats.json \\\n",
        "--epochs 10 \\\n",
        "--nodes 1 \\\n",
        "--gpus 4 \\\n",
        "--alpha 0.6\n",
        "```\n",
        "\n",
        "This will train a model in 10 epochs, with the label priors for the blank token converging from 0.79 to 0.28.\n",
        "\n",
        "The model can be decoded with the same command as above. The CER is 4.98%/5.61% on test-clean and test-other, same as the standard CTC model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osqHG3bkNHQF"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"audio/examples/asr/librispeech_alignment/alignment/checkpoints/ctc_tdnndnn_k2_0.6/epoch=9-step=65369.ckpt\"\n",
        "model_with_priors = AcousticModelModule.load_from_checkpoint(checkpoint_path, tokenizer=tokenizer).eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOd76y9ZNKiT"
      },
      "outputs": [],
      "source": [
        "tokenized_transcript = tokenizer.encode_flatten(transcript, out_type=int)\n",
        "emission2 = model_with_priors.forward(batch)\n",
        "aligned_tokens2, alignment_scores2 = model_with_priors.align(batch, tokenized_transcript)\n",
        "token_spans2 = torchaudio.functional.merge_tokens(aligned_tokens2, alignment_scores2)\n",
        "word_spans2 = unflatten(token_spans2, [len(tokenizer.encode_flatten(word)) for word in transcript.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "m7L_chMzNMgQ",
        "outputId": "afce1114-b303-4737-eb75-daceb617ea7b"
      },
      "outputs": [],
      "source": [
        "plot_alignments(waveform, word_spans2, emission2, transcript.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "3p4YIhizNOyg",
        "outputId": "16fb7ebf-6c7c-49ea-de3c-c73aa9a52395"
      },
      "outputs": [],
      "source": [
        "plot_scores(word_spans2, alignment_scores2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM0Rfi1yN34P"
      },
      "source": [
        "As we can see from the visualization above, the CTC model trained with label priors can produce less peaky posteriors compared to a standard CTC model. We can also listen to each word segments -- the alignments with label priors are more accurate than the standard one (e.g., the words \"i\", \"me\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgVSP9c3R5hw"
      },
      "outputs": [],
      "source": [
        "def preview_word(waveform, spans, num_frames, transcript, sample_rate=16000):\n",
        "    ratio = waveform.size(1) / num_frames\n",
        "    x0 = int(ratio * spans[0].start)\n",
        "    x1 = int(ratio * spans[-1].end)\n",
        "    print(f\"{transcript} ({_score(spans):.2f}): {x0 / sample_rate:.3f} - {x1 / sample_rate:.3f} sec\")\n",
        "    segment = waveform[:, x0:x1]\n",
        "    return IPython.display.Audio(segment.numpy(), rate=sample_rate)\n",
        "\n",
        "\n",
        "num_frames = emission1.size(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "9-AINdT_SDGF",
        "outputId": "953c2811-d088-4924-aafc-53ca4584a50e"
      },
      "outputs": [],
      "source": [
        "# Listen to these words: 0, 5\n",
        "word_idx = 5\n",
        "preview_word(waveform, word_spans1[word_idx], num_frames, transcript.split()[word_idx], sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "is97ELP0SOTI",
        "outputId": "a71ae138-f82b-4a76-a26f-a61ee9bc6108"
      },
      "outputs": [],
      "source": [
        "preview_word(waveform, word_spans2[word_idx], num_frames, transcript.split()[word_idx], sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VedIJRq_r-Yk"
      },
      "source": [
        "### 1.3 Fine-tuning CTC model with label priors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYZQ4REUrcXS"
      },
      "outputs": [],
      "source": [
        "# Coming soon! Basically, we will replicate the results in the paper in this part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uprVwodbr_4i"
      },
      "source": [
        "## Part 2: obtaining robust alignment for long audio and noisy text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bohNgxBqsFDG"
      },
      "source": [
        "In part 1, we performed forced alignment at the utterance level. In practice, we don't usually have a small segment (e.g., 10 seconds) of audio and its corresponding exact, verbatim transcription as in a laboratory setting (e.g. [LIBRISPEECH](https://www.openslr.org/12) corpus). Instead, the audios come in long form (e.g., a whole mp3 recording of speech for an hour). The transcription for the whole recording can by noisy and non-verbatim, which may not exactly match what's been spoken in the recording. In particular, in order to use the raw speech data for machine learning, we usually need to prepare a corpus of segmented audios. In some applications such as automatic captioning, we still hope to align the long audio and text data as much as possible. In this tutorial, we will provide a python library, *TorchAudio Aligner*, to support such use cases.\n",
        "\n",
        "Here, we are facing two challenges:\n",
        "- **The audio is long**, which may not be suitable to be handled as a whole due to, e.g., limited CPU/GPU memory.\n",
        "- **The transcript is long and noisy**. It can be a partial transcript with some missing words. It may have significant errors. It may also contain extra contents that's not spoken in the audio (e.g., the corresponding audio is corrupted). It can be a combination of all cases. Thus, the conventional, basic forced alignment algorithm could provide very bad alignment results, as it assumes the audio and text match exactly.\n",
        "\n",
        "There are a few existing solutions:\n",
        "- [Kaldi](https://ieeexplore.ieee.org/document/8268956), [Gentle](https://github.com/lowerquality/gentle) and [this work](https://ieeexplore.ieee.org/document/7404861) employ a weighted finite state transducer (WFST) framework to model the noisy texts.\n",
        "- [WhisperX](https://github.com/m-bain/whisperX) uses attention mechanism to propose rough time stamps for uniformly segmented audio. Then, it performs phone-level or word-level forced alignment with an external aligner.\n",
        "- [MMS](https://arxiv.org/abs/2305.13516) uses a special `<star>` token to handle missing words in the transcript.\n",
        "- [SailAlign](https://www.semanticscholar.org/paper/SailAlign%3A-Robust-long-speech-text-alignment-Katsamanis-Georgiou/0b7f86429641b188cc62ec32eee590e8795a3d02) iteratively identifies reliable regions and then narrows down to align the remaining unaligned regions.\n",
        "\n",
        "This tutorial is based on WFST and thus falls in the first category. Our implementation is based on PyTorch. Any CTC model in PyTorch can be equipped with our library to become a robust aligner. This makes our aligner distinguish from existing ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDQWOBt4sJpk"
      },
      "source": [
        "### 2.1 Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQtHmK4fszfy"
      },
      "outputs": [],
      "source": [
        "from alignment.torchaudio_k2_aligner import (\n",
        "    uniform_segmentation_with_overlap,\n",
        "    align_segments,\n",
        "    concat_alignments,\n",
        "    get_final_word_alignment,\n",
        "    align,\n",
        "    get_audacity_labels,\n",
        "    get_gentle_visualization,\n",
        ")\n",
        "from alignment.tokenizer import EnglishCharTokenizer\n",
        "from alignment.factor_transducer import make_factor_transducer_word_level_index_with_skip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQfOTLZ0ZAny"
      },
      "source": [
        "We will use a pre-trained Wav2Vec2 model, [torchaudio.pipelines.MMS_FA](https://pytorch.org/audio/main/generated/torchaudio.pipelines.MMS_FA.html#torchaudio.pipelines.MMS_FA), as the acoustic model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAtKXPoRXqob",
        "outputId": "92684cf8-6869-410f-8fa5-b9a85b63c61f"
      },
      "outputs": [],
      "source": [
        "bundle = torchaudio.pipelines.MMS_FA\n",
        "model = bundle.get_model(with_star=False).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQfYSJ27ZCsp",
        "outputId": "d1df14e7-edf0-4197-e90f-931afdc05ca2"
      },
      "outputs": [],
      "source": [
        "LABELS = bundle.get_labels(star=\"*\")\n",
        "DICTIONARY = bundle.get_dict(star=\"*\")\n",
        "\n",
        "print(LABELS)\n",
        "\n",
        "tokenizer = EnglishCharTokenizer(\n",
        "    token2id=DICTIONARY,\n",
        "    blk_token=\"-\",\n",
        "    unk_token=\"*\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxc-v_mlZG8I"
      },
      "source": [
        "### 2.2 Prepare long audio and noisy text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgwrjh6tZI_B"
      },
      "source": [
        "We will demonstrate aligning the whole book, [Walden by Henry David Thoreau](https://www.gutenberg.org/cache/epub/205/pg205-images.html) (of 115K words), with its audiobook chapter (of 30 minutes) in the [LibriVox project](https://librivox.org/walden-by-henry-david-thoreau/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAO3bTrvZErH"
      },
      "outputs": [],
      "source": [
        "# Download the whole book\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/205/pg205-images.html\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "text = soup.get_text()\n",
        "text = text.replace(\"\\r\\n\", \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWfogrPGZKiJ",
        "outputId": "a39b50ed-31c2-4af3-9eb9-36a52c3d0972"
      },
      "outputs": [],
      "source": [
        "# Download a chapter of the audio book.\n",
        "# The MP3 file is of 30 MB, and the download is usually very fast.\n",
        "# You can check this out in your browser -- just go to Chapter 7:\n",
        "# https://librivox.org/walden-by-henry-david-thoreau/\n",
        "# But sometimes it can be also slow due to unknown network issue.\n",
        "!wget https://ia800707.us.archive.org/20/items/walden_librivox/walden_c07.mp3\n",
        "\n",
        "SPEECH_FILE = \"walden_c07.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pFWMZ-AZM_P"
      },
      "outputs": [],
      "source": [
        "# Play the long audio\n",
        "# IPython.display.Audio(SPEECH_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyzkBk3mZPf4",
        "outputId": "df9422a4-575d-4219-8a49-11a27c2267df"
      },
      "outputs": [],
      "source": [
        "# Preview the transcript\n",
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJF5c9QyZRW_",
        "outputId": "eb2de2d0-0faa-4973-943f-cffcb3827871"
      },
      "outputs": [],
      "source": [
        "# Preview the transcript relevant to the audio\n",
        "\n",
        "# In this running example, the whole audio corresponds to\n",
        "# text[271616: 293529]\n",
        "# or\n",
        "# text.split()[49489: 53362]\n",
        "\n",
        "print(text[271400: 271400+1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8JjNz4DZroz"
      },
      "source": [
        "As we can see above, the audio contains a header \"This is a LibriVox recording ...\" which is not transcribed. On the other hand, as we have downloaded the whole book, it contains a lot of extra text that's not spoken in the audio. Obviously, the standard forced alignment algorithm will not work in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGC5UyqvZbA5",
        "outputId": "6a03c8d6-7400-41e8-a514-5c9413364208"
      },
      "outputs": [],
      "source": [
        "# We tokenize the text into the labels of the acoustic model's output\n",
        "text_tokenized = tokenizer.encode(tokenizer.text_normalize(text))\n",
        "\n",
        "print(f\"There are {len(text_tokenized)} words in the text\")\n",
        "\n",
        "# Preview the tokenization results. This corresponds to the beginning of the audio\n",
        "print(tokenizer.decode(text_tokenized)[49489: 49489+15])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYDP3JRDZwlJ"
      },
      "source": [
        "### 2.3 Use WFST to represent the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbaKtFoWZyVg"
      },
      "source": [
        "#### 2.3.1 WFST basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2eWvAOj-YSO"
      },
      "outputs": [],
      "source": [
        "# Coming soon!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdMtaKbzZ0n3"
      },
      "source": [
        "#### 2.3.2 CTC graph, factor transducer and the variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJV7E4bpZ1m8"
      },
      "outputs": [],
      "source": [
        "# Coming soon!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh-bssfLZ5ja"
      },
      "source": [
        "Now, we will represent the whole book of over 100K words into one single WFST decoding graph that allows occasional insertion/deletioin/substitution errors. We set `skip_penalty=-0.5` and `return_penalty=-18.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFW3UVQVZtm2",
        "outputId": "28fac721-d45b-4bf2-d8f2-368284a3d9f6"
      },
      "outputs": [],
      "source": [
        "# Get the decoding graph of the whole book\n",
        "\n",
        "decoding_graph, word_index_sym_tab, token_sym_tab = \\\n",
        "    make_factor_transducer_word_level_index_with_skip(\n",
        "        text_tokenized,\n",
        "        blank_penalty=0,\n",
        "        skip_penalty=-0.5,\n",
        "        return_penalty=-18.0\n",
        "    )\n",
        "decoding_graph = decoding_graph.to(device)\n",
        "\n",
        "print(f\"There are {decoding_graph.shape[0]} nodes and {decoding_graph.num_arcs} arcs in the decoding graph for the text of {len(text_tokenized)} words.\")\n",
        "print(f\"The decoding graph is on device: {decoding_graph.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8FfedyuaA5R"
      },
      "source": [
        "### 2.4 Handle long audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjMyHA8BaPKx"
      },
      "source": [
        "We hope to feed the audio into the neural network to get the frame-wise posteriors (i.e., the emission matrix) over the label vocabulary. However, the audio book chapter is of about 30 minutes. This is too long to feed into the acoustic model all at once.\n",
        "\n",
        "A common practice is to segment the long audio into small overlapping segments. The segments are processed independently and the results are concatenated appropriately to make the final alignment result. Here, as the Wav2Vec2 pretrained model that we use is taking a raw wave form as input, so we will segment the original audio into 15-second segments. In practice, we can also segment the feature vector (e.g., Fbanks) to feed into the acoustic model.\n",
        "\n",
        "The library that we provide in this tutorial provides functions for appropriate segmentation and concatenation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJcTR-AVaT4g"
      },
      "source": [
        "Now, let's use the library to segment the 30-minute audio into 15-second segments, with an overlap of 2 seconds between neighboring segments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxobYT95Z7Y7",
        "outputId": "6d3b1e7e-d4c6-4d83-9f40-d1cacc3b4789"
      },
      "outputs": [],
      "source": [
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
        "resample_rate = 16000  # this is the sample rate of the Wav2Vec2 model\n",
        "waveform = torchaudio.functional.resample(waveform, sample_rate, resample_rate)\n",
        "sample_rate = resample_rate\n",
        "print(waveform.shape, sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMKsessEaVrW",
        "outputId": "dfab7ee0-0680-4a8c-a9cd-b2ecfdd8343b"
      },
      "outputs": [],
      "source": [
        "if waveform.dim() == 2:\n",
        "    waveform.unsqueeze_(-1)\n",
        "segment_size = sample_rate * 15 + 128  # 15 seconds; use extra 128 waveform samples to make sure we have 750 frames for each full-sized segment\n",
        "overlap = sample_rate * 2 + 128        # 2 seconds\n",
        "shortest_segment_size = sample_rate * 0.2  # if the last segment has less than 3200 samples (0.2 seconds), it will be discarded\n",
        "\n",
        "waveform_segmented, segment_lengths, segment_offsets = uniform_segmentation_with_overlap(\n",
        "    waveform,\n",
        "    segment_size,\n",
        "    overlap,\n",
        "    shortest_segment_size=shortest_segment_size\n",
        ")\n",
        "waveform_segmented = waveform_segmented.squeeze()\n",
        "print(waveform_segmented.shape, segment_lengths.shape, segment_offsets.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XukGsAmiacDV"
      },
      "source": [
        "We can listen to a segment to make sure the segmentation pipeline runs correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "PJLaiFpiaZpp",
        "outputId": "0caee07e-1435-47c6-d90b-295bda4d0a61"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(waveform_segmented[0], rate=sample_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7H5EfVXai30"
      },
      "source": [
        "### 2.5 Obtain alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybOakMsbanP4"
      },
      "source": [
        "Now, we will feed the short segments to the neural network based acoustic model. We do this in batches. For each segment, the acoustic model produces frame-wise classification `batch_emissions` over its label vocabulary. This is then combined with the WFST `decoding_graph`, which is shared by all segments, to produce the best alignment path for each segment. If the `decoding_graph` is a linear WFST, which means there is no insertion/deletion/substitution errors in the transcript, this is equivalent to the conventional forced alignment as provided by this [TorchAudio API](https://pytorch.org/audio/main/tutorials/ctc_forced_alignment_api_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ij7R-TgRapH8"
      },
      "source": [
        "There something to note for the `decoding_graph`. When we combine it with the neural network output, its \"input label\" should match the network's label vocabulary. However, it is our decision to define decoding graph's \"output labels\", which basically ask this question: what do we align the audio to? Here are two examples of design choices:\n",
        "\n",
        "- The output labels are word labels or phoneme labels, as in most WFST-based alignment solutions (e.g., in [Gentle](https://github.com/lowerquality/gentle/blob/master/gentle/diff_align.py#L16)). In this case, the alignment paths consist of the word-level or phoneme-level transcripts predicted by the model.\n",
        "\n",
        "- The output labels are the **word indices** in the transcript, instead of word labels. There are two benefits: (1) we can obtain the word labels easily from the word indices; (2) with word indices, we are able to preserve the ordering of words in the transcript, even though the alignment paths may contain words in the order different from the original transcript. As we will see later, this word ordering information enables efficient and effective heuristics for post-processing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gbOiZd_adYN",
        "outputId": "3cf12964-ddc3-4c94-c0b8-d55a72034e93"
      },
      "outputs": [],
      "source": [
        "if device == torch.device(\"cpu\"):\n",
        "    batch_size = 4\n",
        "else:\n",
        "    batch_size = 32\n",
        "\n",
        "output_frames_offset = segment_offsets // (sample_rate * 0.02)\n",
        "\n",
        "alignment_results = list()\n",
        "for i in tqdm(range(0, waveform_segmented.size(0), batch_size)):\n",
        "    batch_waveform_segmented = waveform_segmented[i: i+batch_size].to(device)\n",
        "    batch_segment_lengths = segment_lengths[i: i+batch_size]\n",
        "    batch_output_frames_offset = output_frames_offset[i: i+batch_size]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        # Checkout the API of the forward function here: https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/utils.py#L34\n",
        "        batch_emissions, batch_emissions_lengths = model(batch_waveform_segmented.to(device), batch_segment_lengths.to(device))\n",
        "\n",
        "    # Attach the star dimension manually, see torchaudio issue #3772\n",
        "    star_dim = torch.empty((batch_emissions.size(0), batch_emissions.size(1), 1), device=batch_emissions.device, dtype=batch_emissions.dtype)\n",
        "    star_dim[:] = -5.0\n",
        "    batch_emissions = torch.cat((batch_emissions, star_dim), 2)\n",
        "\n",
        "    # `token_ids` and `timestamps` will each be a list of lists.\n",
        "    # Each sublist corresponds to a segment in the batch.\n",
        "    batch_results = align_segments(\n",
        "        batch_emissions,\n",
        "        decoding_graph,\n",
        "        batch_emissions_lengths,\n",
        "    )\n",
        "\n",
        "    # The interpretation of `token.token_id` depends on the decoding graph.\n",
        "    # Here, in this tutorial, `token.token_id` is the key to the `word_index_sym_tab``\n",
        "    # and `token_sym_tab` dictionaries.\n",
        "    for aligned_tokens, offset in zip(batch_results, batch_output_frames_offset):\n",
        "        for token in aligned_tokens:\n",
        "            token.timestamp += offset  # This will become the absolute frame timestamp in the whole audio\n",
        "            if token.token_id == tokenizer.blk_id:\n",
        "                continue\n",
        "            if token.token_id in word_index_sym_tab:\n",
        "                token.attr[\"wid\"] = word_index_sym_tab[token.token_id]\n",
        "            if token.token_id in token_sym_tab:\n",
        "                token.attr[\"tk\"] = token_sym_tab[token.token_id]\n",
        "\n",
        "    alignment_results.extend(batch_results)\n",
        "\n",
        "    # It will be too slow to do inference on CPU.\n",
        "    # As it's only for the demonstration purpose, so we will break the loop here.\n",
        "    # We can still see some partial alignment results.\n",
        "    if device == torch.device(\"cpu\"):\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jle2za6SbOfj"
      },
      "source": [
        "We've got the alignment results for all 140 15-second segments in less than a minute, thanks to GPU acceleration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae5GpXOvargx",
        "outputId": "59686d1f-34ef-4f34-d807-bc6fde0b1b76"
      },
      "outputs": [],
      "source": [
        "len(alignment_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWKOqR__bRtn"
      },
      "source": [
        "Now we are going to concatenate the alignment results appropriately. We need to take care of two things: (1) the mis-aligned results, (2) the overlapping parts. The common algorithm for this concatenation step is through [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance), which aligns the aligned transcript segments with the ground-truth transcript, e.g., as in [this codes in Gentle](https://github.com/lowerquality/gentle/blob/master/gentle/diff_align.py#L16). As we have word indices instead of word labels, we can do this a bit differently and better. Given the alignment results for all segments, we will find the [longest increasing subsequence (LIS)](https://en.wikipedia.org/wiki/Longest_increasing_subsequence) in the word indices. This can be done in $O(NlogN)$ time complexity and $O(N)$ space complexity, instead of $O(N^2)$ by Levenshtein distance, where $N$ is the total length of the segment-wise alignment results. This can be useful especially when $N$ is large for long audios. The python implementation of efficient LIS algorithm can be found in [this repository](https://github.com/huangruizhe/lis).\n",
        "\n",
        "Moreover, with word indices, we are able to develop heuristics to remove outliers in the alignment results. This has been all implemented in our alignment library. E.g., by calling the `concat_alignments` function below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ex_faZUbP73",
        "outputId": "bc0177a2-82a4-4bae-a950-7b26c8d296c8"
      },
      "outputs": [],
      "source": [
        "# Concatenate the alignment results for segments\n",
        "\n",
        "# `resolved_alignment_results` is a list of `AlignedToken`\n",
        "# `unaligned_text_indices` is a list of (start_word_index, end_word_index)\n",
        "#    which corresponds to \"holes\" in the long text that are not aligned\n",
        "resolved_alignment_results, unaligned_text_indices = concat_alignments(\n",
        "    alignment_results,\n",
        "    neighborhood_size=5,\n",
        ")\n",
        "\n",
        "len(resolved_alignment_results), len(unaligned_text_indices)\n",
        "\n",
        "# We can check the results in `resolved_alignment_results`:\n",
        "# text_splitted = text.split()\n",
        "# [text_splitted[tk.attr['wid']] if 'wid' in tk.attr else tokenizer.id2token[tk.attr['tk']] for tk in resolved_alignment_results[-40:-1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K05Mg_hQbupB"
      },
      "source": [
        "Our final word-level alignment results can be obtained by the following, where `word_alignment` is a dictionary from word indices in `text` to an `AlignedWord` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWLsJCOzbrRk"
      },
      "outputs": [],
      "source": [
        "# `word_alignment` is a dict of word index in the long text => AlignedWord object\n",
        "word_alignment = get_final_word_alignment(resolved_alignment_results, text, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3rTvMNLbyFj"
      },
      "source": [
        "Finally, let's preview the alignment results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyU2gy_2bwUu",
        "outputId": "35492f85-3488-4275-943a-1f03fa2a323a"
      },
      "outputs": [],
      "source": [
        "list(word_alignment.items())[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIBG4kpbbzys",
        "outputId": "e03f3d85-a1db-45a1-e158-20a6831d01b7"
      },
      "outputs": [],
      "source": [
        "list(word_alignment.items())[-10:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ZtFc8Ub4WK"
      },
      "source": [
        "As we can see from above, the alignment results look pretty good! Despite the book being long and noisy, it successfully locates [the chapter](https://www.gutenberg.org/cache/epub/205/pg205-images.html#chap08) from the whole book corresponding to the audio, except the first two words (\"The Bean-Field\") which are the chapter title. We will do some visualization to verify this results later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p--GukFcFA0"
      },
      "source": [
        "On the other hand, in `unaligned_text_indices`, we can see the parts in the book chapter that are not successfully aligned to the audio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqeqNNNHb1y8",
        "outputId": "fcf35c1e-52f6-45c3-ad3f-ffe5f328b0b6"
      },
      "outputs": [],
      "source": [
        "# The range [s, e] (both inclusive) is a \"hole\" in the long text that is not aligned\n",
        "# where s and e are the word indices in the long text\n",
        "text_splitted = text.split()\n",
        "print(f\"There are {len([None for s, e in unaligned_text_indices if e - s > 0])} holes of size greater than one word, out of {len(unaligned_text_indices)} holes in total.\\n\")\n",
        "for s, e in unaligned_text_indices:\n",
        "    if e - s > 0: # ignore the \"holes\" of only one word\n",
        "        print(f\"Unaligned text range: [{s}, {e}]\")\n",
        "        print(f\"Unaligned text: {text_splitted[s:e+1]}\")\n",
        "        print(f\"Time range: {word_alignment[s-1].start_time * 0.02:.2f} ~ {word_alignment[e+1].start_time * 0.02:.2f} seconds\")\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pruqGooVcUHc"
      },
      "source": [
        "### 2.6 Visualization of alignment results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXgLNJVKca3Y"
      },
      "source": [
        "Finally, we can listen to and visually check the alignment results. We will provide two visualization with [Audacity](https://www.audacityteam.org/) and [Gentle](http://lowerquality.com/gentle/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQSasdg8MrR3"
      },
      "source": [
        "#### 2.6.1 Play a segment of the audio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQTn0m0PO21e"
      },
      "source": [
        "To verify the alignment results, we can take a segment of the long audio and see if it matches its alignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvCREcjVT0G6",
        "outputId": "4c56993c-49ab-4b0b-ba71-6f8eb3ce9fdd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# choose a random position in the word-level alignment results\n",
        "i = random.randint(0, len(word_alignment) - 1)\n",
        "word_alignment_segment = list(word_alignment.items())[i: i + 50]\n",
        "t1 = word_alignment_segment[0][1].start_time.item() * 0.02\n",
        "t2 = word_alignment_segment[-1][1].start_time.item() * 0.02\n",
        "t1, t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "a0sZjflmU_gW",
        "outputId": "d22f4586-c219-492e-b561-b99098219716"
      },
      "outputs": [],
      "source": [
        "text_segment = text_splitted[word_alignment_segment[0][0]: word_alignment_segment[-1][0]]\n",
        "\" \".join(text_segment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "92vMrMSvUpIZ",
        "outputId": "668001be-25e8-44a2-841f-558eef3d5780"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "audio_segment = AudioSegment.from_file(SPEECH_FILE)\n",
        "audio_segment = audio_segment[t1 * 1000: t2 * 1000]\n",
        "\n",
        "IPython.display.Audio(audio_segment.get_array_of_samples(), rate=audio_segment.frame_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht7p2J7XWapO"
      },
      "source": [
        "We can see that the segmentation is pretty accurate based on the timestamps we obtained from the alignment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FENj9kj2cj1U"
      },
      "source": [
        "#### 2.6.2 Audacity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vyl86X8cgIi"
      },
      "source": [
        "For Audacity, we will provide the label file that can be imported to the software along with the audio file. Follow the instructions [here](https://manual.audacityteam.org/man/importing_and_exporting_labels.html) to import labels.\n",
        "\n",
        "<img src=\"https://i.postimg.cc/3wpFfSJV/Screen-Shot-2024-04-18-at-4-40-44-AM.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y09dIMEecRW6"
      },
      "outputs": [],
      "source": [
        "audacity_labels_str = get_audacity_labels(word_alignment, frame_duration=0.02)\n",
        "\n",
        "audacity_path = str(SPEECH_FILE)[:-4] + \"_audacity.txt\"\n",
        "with open(audacity_path, \"w\") as fout:\n",
        "    print(audacity_labels_str, file=fout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6y3J8szcplr"
      },
      "source": [
        "#### 2.6.3 Gentle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL61SlQ1cu-6"
      },
      "source": [
        "Gentle comes with a nice, handy [visualization mechanism](https://github.com/lowerquality/gentle/blob/master/serve.py#L113) based on HTML and JavaScript. The following function will generate the necessary files (the \"index.html\" file) and instructions for visualization in Gentle.\n",
        "\n",
        "<img src=\"https://i.postimg.cc/xd3rLCQk/Screen-Shot-2024-04-18-at-4-34-40-AM.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z7Zo37kchre",
        "outputId": "291bcfe7-32a8-4b22-9db9-8792db1768d0"
      },
      "outputs": [],
      "source": [
        "get_gentle_visualization(word_alignment, tokenizer, frame_duration=0.02, audio_file=SPEECH_FILE, text=text, i_word_start=49489, i_word_end=53362)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59xZjzrsNCJT"
      },
      "source": [
        "### 2.7 Multilingual support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jx6BGTaENUEO"
      },
      "source": [
        "According to [this tutorial](https://pytorch.org/audio/stable/tutorials/forced_alignment_for_multilingual_data_tutorial.html), we can use the same MMS_FA model that we have used above to align long audio and noisy text in other languages -- MMS_FA supports 1100+ languages! Other multiligual models, such as [XLSR](https://huggingface.co/masoudmzb/wav2vec2-xlsr-multilingual-53-fa), can be considered as well. Let's demonstrate this with some examples. We choose books in other languages from [LibriVox](https://librivox.org/search?primary_key=0&search_category=language&search_page=1&search_form=get_results&search_order=alpha). We will align audio to the book. In the end, we will listen to the alignment segments to see if they are aligned correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-3w1xGWkDr-"
      },
      "source": [
        "#### 2.7.1 Portuguese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dikxUyPVm046"
      },
      "source": [
        "We take this chapter as the example: \"46 Ode Triunfal\" in [Orpheu no.1](https://librivox.org/orpheu-no1/). This audio book chapter is of 17 minutes long. The [book](https://www.gutenberg.org/cache/epub/23620/pg23620-images.html) has about 18K words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDD1613mcw4W"
      },
      "outputs": [],
      "source": [
        "# Download audio\n",
        "\n",
        "!wget https://ia801705.us.archive.org/7/items/orpheu_no1_2010_librivox/orpheuno1_46__128kb.mp3\n",
        "SPEECH_FILE = \"orpheuno1_46__128kb.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eaw2sjJMaqWf"
      },
      "outputs": [],
      "source": [
        "# Download text\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/23620/pg23620-images.html\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "text = soup.get_text()\n",
        "text = text.replace(\"\\r\\n\", \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehqP1_myeytP",
        "outputId": "cee390b4-c708-4f29-b582-2d732bc2a66c"
      },
      "outputs": [],
      "source": [
        "# Preview text\n",
        "\n",
        "print(text[10000:11000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8ciUYxjh_nu",
        "outputId": "71b68117-1139-48c9-b615-bfc8d35fbb8b"
      },
      "outputs": [],
      "source": [
        "!pip install uroman-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbgDVUZWe4Zl",
        "outputId": "192dedee-4952-487b-8698-e0c2a62d471c"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "\n",
        "import uroman\n",
        "import re\n",
        "\n",
        "def normalize_uroman(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace(\"\", \"'\")\n",
        "    text = re.sub(\"([^a-z' ])\", \" \", text)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "text_romanized = uroman.uroman(text, language=\"por\")\n",
        "text_normalized = normalize_uroman(text_romanized)\n",
        "text_tokenized = tokenizer.encode(tokenizer.text_normalize(text_normalized))\n",
        "\n",
        "print(f\"There are {len(text_tokenized)} words in the text\")\n",
        "\n",
        "# Preview the tokenization results. This corresponds to the beginning of the audio\n",
        "print(tokenizer.decode(text_tokenized)[14444: 14444+15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxXEDC7Of0k3",
        "outputId": "1067cb4b-140c-4468-cd06-2f81e386ad69"
      },
      "outputs": [],
      "source": [
        "# Get the decoding graph of the whole book\n",
        "\n",
        "decoding_graph, word_index_sym_tab, token_sym_tab = \\\n",
        "    make_factor_transducer_word_level_index_with_skip(\n",
        "        text_tokenized,\n",
        "        blank_penalty=0,\n",
        "        skip_penalty=-0.5,\n",
        "        return_penalty=-18.0\n",
        "    )\n",
        "decoding_graph = decoding_graph.to(device)\n",
        "\n",
        "print(f\"There are {decoding_graph.shape[0]} nodes and {decoding_graph.num_arcs} arcs in the decoding graph for the text of {len(text_tokenized)} words.\")\n",
        "print(f\"The decoding graph is on device: {decoding_graph.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n-34k6if5sD",
        "outputId": "251021f2-38fd-43b6-f8fb-bae4998c165e"
      },
      "outputs": [],
      "source": [
        "# Handle long audio\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
        "resample_rate = 16000  # this is the sample rate of the Wav2Vec2 model\n",
        "waveform = torchaudio.functional.resample(waveform, sample_rate, resample_rate)\n",
        "sample_rate = resample_rate\n",
        "print(waveform.shape, sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a7PixqJiozd",
        "outputId": "60098d55-80f8-4ee2-f759-db8c831ddb6d"
      },
      "outputs": [],
      "source": [
        "# Uniform segmentation\n",
        "\n",
        "if waveform.dim() == 2:\n",
        "    waveform.unsqueeze_(-1)\n",
        "segment_size = sample_rate * 15 + 128  # 15 seconds; use extra 128 waveform samples to make sure we have 750 frames for each full-sized segment\n",
        "overlap = sample_rate * 2 + 128        # 2 seconds\n",
        "shortest_segment_size = sample_rate * 0.2  # if the last segment has less than 3200 samples (0.2 seconds), it will be discarded\n",
        "\n",
        "waveform_segmented, segment_lengths, segment_offsets = uniform_segmentation_with_overlap(\n",
        "    waveform,\n",
        "    segment_size,\n",
        "    overlap,\n",
        "    shortest_segment_size=shortest_segment_size\n",
        ")\n",
        "waveform_segmented = waveform_segmented.squeeze()\n",
        "print(waveform_segmented.shape, segment_lengths.shape, segment_offsets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "-kvLgBC6jJEc",
        "outputId": "504d11d0-1b8c-4b31-97c7-e72a65169d14"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(waveform_segmented[0], rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVtsFqq1jQ9t",
        "outputId": "c653052c-ae59-4712-89a6-1bfa7850b948"
      },
      "outputs": [],
      "source": [
        "# Obtain alignment for segments\n",
        "\n",
        "if device == torch.device(\"cpu\"):\n",
        "    batch_size = 4\n",
        "else:\n",
        "    batch_size = 32\n",
        "\n",
        "output_frames_offset = segment_offsets // (sample_rate * 0.02)\n",
        "\n",
        "alignment_results = list()\n",
        "for i in tqdm(range(0, waveform_segmented.size(0), batch_size)):\n",
        "    batch_waveform_segmented = waveform_segmented[i: i+batch_size].to(device)\n",
        "    batch_segment_lengths = segment_lengths[i: i+batch_size]\n",
        "    batch_output_frames_offset = output_frames_offset[i: i+batch_size]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        # Checkout the API of the forward function here: https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/utils.py#L34\n",
        "        batch_emissions, batch_emissions_lengths = model(batch_waveform_segmented.to(device), batch_segment_lengths.to(device))\n",
        "\n",
        "    # Attach the star dimension manually, see torchaudio issue #3772\n",
        "    star_dim = torch.empty((batch_emissions.size(0), batch_emissions.size(1), 1), device=batch_emissions.device, dtype=batch_emissions.dtype)\n",
        "    star_dim[:] = -5.0\n",
        "    batch_emissions = torch.cat((batch_emissions, star_dim), 2)\n",
        "\n",
        "    # `token_ids` and `timestamps` will each be a list of lists.\n",
        "    # Each sublist corresponds to a segment in the batch.\n",
        "    batch_results = align_segments(\n",
        "        batch_emissions,\n",
        "        decoding_graph,\n",
        "        batch_emissions_lengths,\n",
        "    )\n",
        "\n",
        "    # The interpretation of `token.token_id` depends on the decoding graph.\n",
        "    # Here, in this tutorial, `token.token_id` is the key to the `word_index_sym_tab``\n",
        "    # and `token_sym_tab` dictionaries.\n",
        "    for aligned_tokens, offset in zip(batch_results, batch_output_frames_offset):\n",
        "        for token in aligned_tokens:\n",
        "            token.timestamp += offset  # This will become the absolute frame timestamp in the whole audio\n",
        "            if token.token_id == tokenizer.blk_id:\n",
        "                continue\n",
        "            if token.token_id in word_index_sym_tab:\n",
        "                token.attr[\"wid\"] = word_index_sym_tab[token.token_id]\n",
        "            if token.token_id in token_sym_tab:\n",
        "                token.attr[\"tk\"] = token_sym_tab[token.token_id]\n",
        "\n",
        "    alignment_results.extend(batch_results)\n",
        "\n",
        "    # It will be too slow to do inference on CPU.\n",
        "    # As it's only for the demonstration purpose, so we will break the loop here.\n",
        "    # We can still see some partial alignment results.\n",
        "    if device == torch.device(\"cpu\"):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxU7jpgWjXQK",
        "outputId": "7d4e6367-ed2b-44a1-dd3f-efedfd1f2112"
      },
      "outputs": [],
      "source": [
        "# Concatenate the alignment results for segments\n",
        "\n",
        "# `resolved_alignment_results` is a list of `AlignedToken`\n",
        "# `unaligned_text_indices` is a list of (start_word_index, end_word_index)\n",
        "#    which corresponds to \"holes\" in the long text that are not aligned\n",
        "resolved_alignment_results, unaligned_text_indices = concat_alignments(\n",
        "    alignment_results,\n",
        "    neighborhood_size=5,\n",
        ")\n",
        "\n",
        "len(resolved_alignment_results), len(unaligned_text_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXRH1FpIjcPa"
      },
      "outputs": [],
      "source": [
        "# Get the final word-level alignments\n",
        "\n",
        "# `word_alignment` is a dict of word index in the long text => AlignedWord object\n",
        "word_alignment = get_final_word_alignment(resolved_alignment_results, text_normalized, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlIL063LjhTv",
        "outputId": "14762383-a54c-4203-a13f-f2997f7ff0d3"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# choose a random position in the word-level alignment results\n",
        "i = random.randint(0, len(word_alignment) - 1)\n",
        "word_alignment_segment = list(word_alignment.items())[i: i + 50]\n",
        "t1 = word_alignment_segment[0][1].start_time.item() * 0.02\n",
        "t2 = word_alignment_segment[-1][1].start_time.item() * 0.02\n",
        "t1, t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "3gJGFPoGjlVM",
        "outputId": "b2729b17-3c92-46f2-db78-f61dd6c438dd"
      },
      "outputs": [],
      "source": [
        "text_splitted = text_normalized.split()\n",
        "text_segment = text_splitted[word_alignment_segment[0][0]: word_alignment_segment[-1][0]]\n",
        "\" \".join(text_segment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "-u2_jMr6jukU",
        "outputId": "dd16d0f9-ab82-4c1a-b458-4bc80bf5f078"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "audio_segment = AudioSegment.from_file(SPEECH_FILE)\n",
        "audio_segment = audio_segment[t1 * 1000: t2 * 1000]\n",
        "\n",
        "IPython.display.Audio(audio_segment.get_array_of_samples(), rate=audio_segment.frame_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmhvmNVQkLew"
      },
      "source": [
        "#### 2.7.2 Chinese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2V4DVJuUmioc"
      },
      "source": [
        "We take this chapter as the example: \"14  \" in [ Lun Yu (Analects of Confucius)](https://librivox.org/lun-yu-or-analects-of-confucius-read-in-chinese/). This audio book chapter is of 11.5 minutes long. The whole book has about 15K Chinese characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y1vmw02kTzI",
        "outputId": "48ed73fd-b255-4964-d15d-0a3647b4a25b"
      },
      "outputs": [],
      "source": [
        "# Download audio\n",
        "\n",
        "!wget https://ia801307.us.archive.org/15/items/lun_yu_0801_librivox/lunyu_14_confucius.mp3\n",
        "SPEECH_FILE = \"lunyu_14_confucius.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soAEYcQvkPd0",
        "outputId": "3a78d014-d9ca-4db9-8c1c-f9f44b1257c8"
      },
      "outputs": [],
      "source": [
        "# Download text\n",
        "\n",
        "![ ! -d \"The-Analects\" ] && git clone https://github.com/Muzi828/The-Analects.git\n",
        "\n",
        "with open('The-Analects/-.txt', 'r', encoding='gbk') as f:\n",
        "    # Read the entire file as a string\n",
        "    text = f.read()\n",
        "text = text.replace(\"\\r\\n\", \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahwuowaSpJVZ",
        "outputId": "94b29ced-3169-494d-848b-ffb44bf5992a"
      },
      "outputs": [],
      "source": [
        "# Preview text\n",
        "\n",
        "print(text[:200])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMZQrSd7sRIe",
        "outputId": "94227402-05cd-426e-c9a3-7587baea5bc5"
      },
      "outputs": [],
      "source": [
        "!pip install zhon uroman-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxdRmPsBp7wC",
        "outputId": "8b6232d0-6968-47a6-a95e-5e87ca247a9e"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "\n",
        "import uroman\n",
        "import re\n",
        "import zhon\n",
        "import string\n",
        "\n",
        "punctuation_set = set(zhon.hanzi.punctuation + string.punctuation)\n",
        "\n",
        "def remove_punctuation_and_split_characters(text):\n",
        "    text = \"\".join(text.split())\n",
        "    text = \"\".join([c for c in text if c not in punctuation_set])\n",
        "    return \" \".join(list(text))\n",
        "\n",
        "text_normalized = remove_punctuation_and_split_characters(text)\n",
        "text_romanized = uroman.uroman(text_normalized, language=\"cmn\")\n",
        "text_tokenized = tokenizer.encode(tokenizer.text_normalize(text_romanized))\n",
        "\n",
        "assert len(text_normalized.split()) == len(text_romanized.split()) == len(text_tokenized)\n",
        "\n",
        "print(f\"There are {len(text_tokenized)} words in the text\")\n",
        "\n",
        "# Preview the tokenization results.\n",
        "print(list(zip(tokenizer.decode(text_tokenized), text_normalized.split()))[144: 144+15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVUjzoQxv6UB",
        "outputId": "a096eef9-8143-4c9b-8cd0-e50017afd8c1"
      },
      "outputs": [],
      "source": [
        "# Get the decoding graph of the whole book\n",
        "\n",
        "decoding_graph, word_index_sym_tab, token_sym_tab = \\\n",
        "    make_factor_transducer_word_level_index_with_skip(\n",
        "        text_tokenized,\n",
        "        blank_penalty=0,\n",
        "        skip_penalty=-0.5,\n",
        "        return_penalty=-18.0\n",
        "    )\n",
        "decoding_graph = decoding_graph.to(device)\n",
        "\n",
        "print(f\"There are {decoding_graph.shape[0]} nodes and {decoding_graph.num_arcs} arcs in the decoding graph for the text of {len(text_tokenized)} words.\")\n",
        "print(f\"The decoding graph is on device: {decoding_graph.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9r4dmvSo0nUR",
        "outputId": "6d30efae-adb6-4b73-a748-89ec9511dcc1"
      },
      "outputs": [],
      "source": [
        "# Handle long audio\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
        "resample_rate = 16000  # this is the sample rate of the Wav2Vec2 model\n",
        "waveform = torchaudio.functional.resample(waveform, sample_rate, resample_rate)\n",
        "sample_rate = resample_rate\n",
        "print(waveform.shape, sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQT-Id4M0pQm",
        "outputId": "651b4b34-38aa-42b8-c4df-51499efa66f3"
      },
      "outputs": [],
      "source": [
        "# Uniform segmentation\n",
        "\n",
        "if waveform.dim() == 2:\n",
        "    waveform.unsqueeze_(-1)\n",
        "segment_size = sample_rate * 15 + 128  # 15 seconds; use extra 128 waveform samples to make sure we have 750 frames for each full-sized segment\n",
        "overlap = sample_rate * 2 + 128        # 2 seconds\n",
        "shortest_segment_size = sample_rate * 0.2  # if the last segment has less than 3200 samples (0.2 seconds), it will be discarded\n",
        "\n",
        "waveform_segmented, segment_lengths, segment_offsets = uniform_segmentation_with_overlap(\n",
        "    waveform,\n",
        "    segment_size,\n",
        "    overlap,\n",
        "    shortest_segment_size=shortest_segment_size\n",
        ")\n",
        "waveform_segmented = waveform_segmented.squeeze()\n",
        "print(waveform_segmented.shape, segment_lengths.shape, segment_offsets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "at_y941E0sFt",
        "outputId": "47263ba3-26cb-442b-9345-a96b4b6fc1d0"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(waveform_segmented[0], rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9E2M0tL0t7j",
        "outputId": "2454158e-be05-4b3b-a364-e8beaa6513a7"
      },
      "outputs": [],
      "source": [
        "# Obtain alignment for segments\n",
        "\n",
        "if device == torch.device(\"cpu\"):\n",
        "    batch_size = 4\n",
        "else:\n",
        "    batch_size = 32\n",
        "\n",
        "output_frames_offset = segment_offsets // (sample_rate * 0.02)\n",
        "\n",
        "alignment_results = list()\n",
        "for i in tqdm(range(0, waveform_segmented.size(0), batch_size)):\n",
        "    batch_waveform_segmented = waveform_segmented[i: i+batch_size].to(device)\n",
        "    batch_segment_lengths = segment_lengths[i: i+batch_size]\n",
        "    batch_output_frames_offset = output_frames_offset[i: i+batch_size]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        # Checkout the API of the forward function here: https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/utils.py#L34\n",
        "        batch_emissions, batch_emissions_lengths = model(batch_waveform_segmented.to(device), batch_segment_lengths.to(device))\n",
        "\n",
        "    # Attach the star dimension manually, see torchaudio issue #3772\n",
        "    star_dim = torch.empty((batch_emissions.size(0), batch_emissions.size(1), 1), device=batch_emissions.device, dtype=batch_emissions.dtype)\n",
        "    star_dim[:] = -5.0\n",
        "    batch_emissions = torch.cat((batch_emissions, star_dim), 2)\n",
        "\n",
        "    # `token_ids` and `timestamps` will each be a list of lists.\n",
        "    # Each sublist corresponds to a segment in the batch.\n",
        "    batch_results = align_segments(\n",
        "        batch_emissions,\n",
        "        decoding_graph,\n",
        "        batch_emissions_lengths,\n",
        "    )\n",
        "\n",
        "    # The interpretation of `token.token_id` depends on the decoding graph.\n",
        "    # Here, in this tutorial, `token.token_id` is the key to the `word_index_sym_tab``\n",
        "    # and `token_sym_tab` dictionaries.\n",
        "    for aligned_tokens, offset in zip(batch_results, batch_output_frames_offset):\n",
        "        for token in aligned_tokens:\n",
        "            token.timestamp += offset  # This will become the absolute frame timestamp in the whole audio\n",
        "            if token.token_id == tokenizer.blk_id:\n",
        "                continue\n",
        "            if token.token_id in word_index_sym_tab:\n",
        "                token.attr[\"wid\"] = word_index_sym_tab[token.token_id]\n",
        "            if token.token_id in token_sym_tab:\n",
        "                token.attr[\"tk\"] = token_sym_tab[token.token_id]\n",
        "\n",
        "    alignment_results.extend(batch_results)\n",
        "\n",
        "    # It will be too slow to do inference on CPU.\n",
        "    # As it's only for the demonstration purpose, so we will break the loop here.\n",
        "    # We can still see some partial alignment results.\n",
        "    if device == torch.device(\"cpu\"):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foX-JBhP0zBp",
        "outputId": "0455f3f9-b06e-4f38-d97f-e94e5742c1f3"
      },
      "outputs": [],
      "source": [
        "# Concatenate the alignment results for segments\n",
        "\n",
        "# `resolved_alignment_results` is a list of `AlignedToken`\n",
        "# `unaligned_text_indices` is a list of (start_word_index, end_word_index)\n",
        "#    which corresponds to \"holes\" in the long text that are not aligned\n",
        "resolved_alignment_results, unaligned_text_indices = concat_alignments(\n",
        "    alignment_results,\n",
        "    neighborhood_size=5,\n",
        ")\n",
        "\n",
        "len(resolved_alignment_results), len(unaligned_text_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaWuLPDi03OH"
      },
      "outputs": [],
      "source": [
        "# Get the final word-level alignments\n",
        "\n",
        "# `word_alignment` is a dict of word index in the long text => AlignedWord object\n",
        "word_alignment = get_final_word_alignment(resolved_alignment_results, text_normalized, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ReLQ29V0-6P",
        "outputId": "2bd53895-dd6a-4000-8965-8231eb5b83e5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# choose a random position in the word-level alignment results\n",
        "i = random.randint(0, len(word_alignment) - 1)\n",
        "word_alignment_segment = list(word_alignment.items())[i: i + 50]\n",
        "t1 = word_alignment_segment[0][1].start_time.item() * 0.02\n",
        "t2 = word_alignment_segment[-1][1].start_time.item() * 0.02\n",
        "t1, t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "-2jJSL2K1B6d",
        "outputId": "05b76c4e-a3a3-4658-ab4b-ada698ccb487"
      },
      "outputs": [],
      "source": [
        "text_splitted = text_normalized.split()\n",
        "text_segment = text_splitted[word_alignment_segment[0][0]: word_alignment_segment[-1][0]]\n",
        "\" \".join(text_segment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "83b6hJL61kb4",
        "outputId": "9a11f894-0344-4a9d-bfe1-6aa6dc7e449c"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "audio_segment = AudioSegment.from_file(SPEECH_FILE)\n",
        "audio_segment = audio_segment[t1 * 1000: t2 * 1000]\n",
        "\n",
        "IPython.display.Audio(audio_segment.get_array_of_samples(), rate=audio_segment.frame_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5MQcquIx9od"
      },
      "source": [
        "#### 2.7.3 Japanese"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHYm3vmsyCbz"
      },
      "source": [
        "We take this chapter as the example: \"03 \" in [ (Kaze Tachinu)\n",
        "](https://librivox.org/kaze-tachinu-by-hori-tatsuo/). This audio book chapter is of 57.5 minutes long. The [whole book](https://www.aozora.gr.jp/cards/001030/files/4803_14204.html) has about 57K Japanese characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZX3eW7-q1mJz",
        "outputId": "944dd353-5e09-4ddb-b288-a28ca584d522"
      },
      "outputs": [],
      "source": [
        "# Download audio\n",
        "\n",
        "!wget https://ia803207.us.archive.org/30/items/kazetachinu_ek_librivox/kazetachinu_03_hori.mp3\n",
        "SPEECH_FILE = \"kazetachinu_03_hori.mp3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        },
        "id": "XWJbUvLqx8lJ",
        "outputId": "9a4dfa41-7619-42eb-c0e6-d3e37b177ac5"
      },
      "outputs": [],
      "source": [
        "# Download text\n",
        "\n",
        "# We need to parse a webpage in Japanese. We need to resolve the encoding correctly\n",
        "# https://github.com/TokyoPython/decodeNihongo/tree/master\n",
        "\n",
        "def retrieveURL(URL):\n",
        "    import urllib.request\n",
        "    import urllib.parse\n",
        "    response = None\n",
        "    reqfull = urllib.request.Request(URL)  #create Post request\n",
        "    try:\n",
        "        response = urllib.request.urlopen(reqfull)\n",
        "    except urllib.request.HTTPError as err:\n",
        "        if err.code == 404:\n",
        "            print(\"URL not found 404 \" + str(URL))\n",
        "        else:\n",
        "            print(\"URL not found \" + str(err.code)+\" \"+ str(URL))\n",
        "    return response\n",
        "\n",
        "def get_decodedNihongo_PageText(URL):  #various encodings of Japanese-language web pages\n",
        "    import html  #To unescape webpage content\n",
        "    # Transmit request, and read first page of results\n",
        "    response = retrieveURL(URL)\n",
        "    if response != None:\n",
        "        responseHTML = response.read()\n",
        "        try:\n",
        "            text = responseHTML.decode('utf-8')  #standard (utf-8 encoding)\n",
        "        except:\n",
        "            try:\n",
        "                text = responseHTML.decode('shiftjis')  #most likely alternate Japanese encoding\n",
        "            except:\n",
        "                text = responseHTML.decode('shift_jisx0213') #be prepared for other Japanese encodings\n",
        "        text = html.unescape(text)  #get rid of \"&quot:\", \"&lt;\", \"&gt;\",etc.\n",
        "    return text\n",
        "\n",
        "\n",
        "response_text = get_decodedNihongo_PageText(\"https://www.aozora.gr.jp/cards/001030/files/4803_14204.html\")\n",
        "soup = BeautifulSoup(response_text, \"html.parser\")\n",
        "\n",
        "text = soup.get_text()\n",
        "text = text.replace(\"\\r\\n\", \"\\n\")\n",
        "text[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02H1U63TB8Kf",
        "outputId": "06f3af14-692f-4109-bc98-b8855ffa0c6a"
      },
      "outputs": [],
      "source": [
        "# For Japanese romanization\n",
        "!pip install cutlet unidic-lite zhon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8xvAuXl71WA",
        "outputId": "31867e21-371e-4e84-b8a1-ae115c1bfaaf"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "\n",
        "import re\n",
        "import zhon\n",
        "import string\n",
        "import cutlet\n",
        "\n",
        "punctuation_set = set(zhon.hanzi.punctuation + string.punctuation)\n",
        "\n",
        "def remove_punctuation_and_split_characters(text):\n",
        "    text = \"\".join(text.split())\n",
        "    text = \"\".join([c for c in text if c not in punctuation_set])\n",
        "    return \" \".join(list(text))\n",
        "\n",
        "katsu = cutlet.Cutlet()\n",
        "\n",
        "text_normalized = remove_punctuation_and_split_characters(text)\n",
        "text_romanized = [katsu.romaji(w) for w in text_normalized.split()]\n",
        "text_romanized = \" \".join([w if w else \"*\" for w in text_romanized])\n",
        "text_tokenized = tokenizer.encode(tokenizer.text_normalize(text_romanized))\n",
        "\n",
        "assert len(text_normalized.split()) == len(text_romanized.split()) == len(text_tokenized)\n",
        "\n",
        "print(f\"There are {len(text_tokenized)} words in the text\")\n",
        "\n",
        "# Preview the tokenization results.\n",
        "print(list(zip(tokenizer.decode(text_tokenized), text_normalized.split()))[144: 144+15])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpibv97NDdc1",
        "outputId": "cf6ac7bc-6848-4fbd-8991-05cabfdf5e26"
      },
      "outputs": [],
      "source": [
        "# Get the decoding graph of the whole book\n",
        "\n",
        "decoding_graph, word_index_sym_tab, token_sym_tab = \\\n",
        "    make_factor_transducer_word_level_index_with_skip(\n",
        "        text_tokenized,\n",
        "        blank_penalty=0,\n",
        "        skip_penalty=-0.5,\n",
        "        return_penalty=-18.0\n",
        "    )\n",
        "decoding_graph = decoding_graph.to(device)\n",
        "\n",
        "print(f\"There are {decoding_graph.shape[0]} nodes and {decoding_graph.num_arcs} arcs in the decoding graph for the text of {len(text_tokenized)} words.\")\n",
        "print(f\"The decoding graph is on device: {decoding_graph.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoayUz6iExC1",
        "outputId": "13177624-57fc-471a-a933-bb0df3626f16"
      },
      "outputs": [],
      "source": [
        "# Handle long audio\n",
        "\n",
        "waveform, sample_rate = torchaudio.load(SPEECH_FILE)\n",
        "resample_rate = 16000  # this is the sample rate of the Wav2Vec2 model\n",
        "waveform = torchaudio.functional.resample(waveform, sample_rate, resample_rate)\n",
        "sample_rate = resample_rate\n",
        "print(waveform.shape, sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NU7f5iG5EzTq",
        "outputId": "7e052342-bd86-482f-e86d-18fcd1051cd7"
      },
      "outputs": [],
      "source": [
        "# Uniform segmentation\n",
        "\n",
        "if waveform.dim() == 2:\n",
        "    waveform.unsqueeze_(-1)\n",
        "segment_size = sample_rate * 15 + 128  # 15 seconds; use extra 128 waveform samples to make sure we have 750 frames for each full-sized segment\n",
        "overlap = sample_rate * 2 + 128        # 2 seconds\n",
        "shortest_segment_size = sample_rate * 0.2  # if the last segment has less than 3200 samples (0.2 seconds), it will be discarded\n",
        "\n",
        "waveform_segmented, segment_lengths, segment_offsets = uniform_segmentation_with_overlap(\n",
        "    waveform,\n",
        "    segment_size,\n",
        "    overlap,\n",
        "    shortest_segment_size=shortest_segment_size\n",
        ")\n",
        "waveform_segmented = waveform_segmented.squeeze()\n",
        "print(waveform_segmented.shape, segment_lengths.shape, segment_offsets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "6ltyJD-xE1cV",
        "outputId": "d0d13ba1-de9c-40d7-8c40-21bb626d17f8"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(waveform_segmented[0], rate=sample_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqs3FVgsE3Ok",
        "outputId": "fa9ba6d8-9f55-49b0-ebb9-1b804030e48a"
      },
      "outputs": [],
      "source": [
        "# Obtain alignment for segments\n",
        "\n",
        "if device == torch.device(\"cpu\"):\n",
        "    batch_size = 4\n",
        "else:\n",
        "    batch_size = 32\n",
        "\n",
        "output_frames_offset = segment_offsets // (sample_rate * 0.02)\n",
        "\n",
        "alignment_results = list()\n",
        "for i in tqdm(range(0, waveform_segmented.size(0), batch_size)):\n",
        "    batch_waveform_segmented = waveform_segmented[i: i+batch_size].to(device)\n",
        "    batch_segment_lengths = segment_lengths[i: i+batch_size]\n",
        "    batch_output_frames_offset = output_frames_offset[i: i+batch_size]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        # Checkout the API of the forward function here: https://github.com/pytorch/audio/blob/main/src/torchaudio/pipelines/_wav2vec2/utils.py#L34\n",
        "        batch_emissions, batch_emissions_lengths = model(batch_waveform_segmented.to(device), batch_segment_lengths.to(device))\n",
        "\n",
        "    # Attach the star dimension manually, see torchaudio issue #3772\n",
        "    star_dim = torch.empty((batch_emissions.size(0), batch_emissions.size(1), 1), device=batch_emissions.device, dtype=batch_emissions.dtype)\n",
        "    star_dim[:] = -5.0\n",
        "    batch_emissions = torch.cat((batch_emissions, star_dim), 2)\n",
        "\n",
        "    # `token_ids` and `timestamps` will each be a list of lists.\n",
        "    # Each sublist corresponds to a segment in the batch.\n",
        "    batch_results = align_segments(\n",
        "        batch_emissions,\n",
        "        decoding_graph,\n",
        "        batch_emissions_lengths,\n",
        "    )\n",
        "\n",
        "    # The interpretation of `token.token_id` depends on the decoding graph.\n",
        "    # Here, in this tutorial, `token.token_id` is the key to the `word_index_sym_tab``\n",
        "    # and `token_sym_tab` dictionaries.\n",
        "    for aligned_tokens, offset in zip(batch_results, batch_output_frames_offset):\n",
        "        for token in aligned_tokens:\n",
        "            token.timestamp += offset  # This will become the absolute frame timestamp in the whole audio\n",
        "            if token.token_id == tokenizer.blk_id:\n",
        "                continue\n",
        "            if token.token_id in word_index_sym_tab:\n",
        "                token.attr[\"wid\"] = word_index_sym_tab[token.token_id]\n",
        "            if token.token_id in token_sym_tab:\n",
        "                token.attr[\"tk\"] = token_sym_tab[token.token_id]\n",
        "\n",
        "    alignment_results.extend(batch_results)\n",
        "\n",
        "    # It will be too slow to do inference on CPU.\n",
        "    # As it's only for the demonstration purpose, so we will break the loop here.\n",
        "    # We can still see some partial alignment results.\n",
        "    if device == torch.device(\"cpu\"):\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuF0RkgnE6XC",
        "outputId": "e33a881d-6734-4ec8-a6a7-8e939a6074ce"
      },
      "outputs": [],
      "source": [
        "# Concatenate the alignment results for segments\n",
        "\n",
        "# `resolved_alignment_results` is a list of `AlignedToken`\n",
        "# `unaligned_text_indices` is a list of (start_word_index, end_word_index)\n",
        "#    which corresponds to \"holes\" in the long text that are not aligned\n",
        "resolved_alignment_results, unaligned_text_indices = concat_alignments(\n",
        "    alignment_results,\n",
        "    neighborhood_size=5,\n",
        ")\n",
        "\n",
        "len(resolved_alignment_results), len(unaligned_text_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mV0UhKzE8hF"
      },
      "outputs": [],
      "source": [
        "# Get the final word-level alignments\n",
        "\n",
        "# `word_alignment` is a dict of word index in the long text => AlignedWord object\n",
        "word_alignment = get_final_word_alignment(resolved_alignment_results, text_normalized, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVE9Sqi2FCML",
        "outputId": "167b7139-f7d9-4810-e656-159bf90ae735"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# choose a random position in the word-level alignment results\n",
        "i = random.randint(0, len(word_alignment) - 1)\n",
        "word_alignment_segment = list(word_alignment.items())[i: i + 50]\n",
        "t1 = word_alignment_segment[0][1].start_time.item() * 0.02\n",
        "t2 = word_alignment_segment[-1][1].start_time.item() * 0.02\n",
        "t1, t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "EUCUgrtBFD6F",
        "outputId": "30ca2bdf-189c-4edb-b5cf-6bcd8fe6aad4"
      },
      "outputs": [],
      "source": [
        "text_splitted = text_normalized.split()\n",
        "text_segment = text_splitted[word_alignment_segment[0][0]: word_alignment_segment[-1][0]]\n",
        "\" \".join(text_segment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "4M_ZU615FFhe",
        "outputId": "f00c5a69-f22a-428a-afc2-29bb00ba50de"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "audio_segment = AudioSegment.from_file(SPEECH_FILE)\n",
        "audio_segment = audio_segment[t1 * 1000: t2 * 1000]\n",
        "\n",
        "IPython.display.Audio(audio_segment.get_array_of_samples(), rate=audio_segment.frame_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRC0bs1yIApT"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjdu43IKQTbT"
      },
      "source": [
        "## TODOs:\n",
        "\n",
        "*   Xiaohui suggested to demonstrate the mechanism to support ins/del/sub errors\n",
        "*   Xiaohui suggested to save the unaligned audio/text and misaligned parts in the final results.\n",
        "*   Let's make an API for multi-lingual support\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpDup3PtQUEW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
